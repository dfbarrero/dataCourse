%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MUW Presentation
% LaTeX Template
% Version 1.0 (27/12/2016)
%
% License:
% CC BY-NC-SA 4.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
% Created by:
% Nicolas Ballarini, CeMSIIS, Medical University of Vienna
% nicoballarini@gmail.com
% http://statistics.msi.meduniwien.ac.at/
%
% Customized for UAH by:
% David F. Barrero, Departamento de Automática, UAH
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[10pt,compress]{beamer} % Change 10pt to make fonts of a different size
\mode<presentation>

\usepackage[spanish]{babel}
\usepackage{fontspec}
\usepackage{tikz}
\usepackage{etoolbox}
\usepackage{xcolor}
\usepackage{xstring}
\usepackage{listings}
\usepackage{multicol}

% Custom packages
\usepackage{algpseudocode}
\usepackage{tikz}
\usepackage{pgfplots}
\def\layersep{2.5cm}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
 

\usetheme{UAH}
\usecolortheme{UAH}
\setbeamertemplate{navigation symbols}{} 
\setbeamertemplate{caption}[numbered]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Presentation Info
\title[Aritificial Neural Networks]{Artificial Neural Networks}
\author{\asignatura\\\carrera}
\institute{}
\date{Departamento de Automática}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Descomentar para habilitar barra de navegación superior
\setNavigation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Configuración de logotipos en portada
%% Opacidad de los logotipos
\newcommand{\opacidad}{1}
%% Descomentar para habilitar logotipo en pié de página de portada
\renewcommand{\logoUno}{Images/isg.png}
%% Descomentar para habilitar logotipo en pié de página de portada
%\renewcommand{\logoDos}{Images/CCLogo.png}
%% Descomentar para habilitar logotipo en pié de página de portada
%\renewcommand{\logoTres}{Images/ALogo.png}
%% Descomentar para habilitar logotipo en pié de página de portada
%\renewcommand{\logoCuatro}{Images/ELogo.png}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% FOOTLINE
%% Comment/Uncomment the following blocks to modify the footline
%% content in the body slides. 


%% Option A: Title and institute
\footlineA
%% Option B: Author and institute
%\footlineB
%% Option C: Title, Author and institute
%\footlineC
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Use this block for a blue title slide with modified footline
{\titlepageBlue
    \begin{frame}
        \titlepage
    \end{frame}
}

\institute{\asignatura}

\begin{frame}[plain]{}
   \begin{block}{Objectives}
      \begin{enumerate}
         \item Describe biological neurons and networks
         \item Describe artifical neurons
	 \item Introduce the MLP
         \item Understand the role of trainning in ANNs
	 \item Understand MLP hyperparameters
      \end{enumerate} 
   \end{block}

   \begin{block}{Bibliography}
	\begin{itemize}
        \item G\'eron, Aur\'elien. \textit{Hands-On Machine Learning with Scikit-Learn, Keras \& TensorFlow}. 2nd Edition. O'Reilly. 2019
	\end{itemize}
   \end{block}
\end{frame}

{
\disableNavigation{white}
\begin{frame}[shrink]{Table of Contents}
 \frametitle{Table of Contents}

    \begin{multicols}{2}
    \tableofcontents
    \end{multicols}
  % You might wish to add the option [pausesections]
\end{frame}
}

\section{Introduction}

%\subsection{Machine Learning tasks}
%\begin{frame}{Introduction}{Machine Learning (I)}
%	Machine Learning studies how to build data-driven models
%	\begin{itemize}
%	\item \textbf{Supervised learning} The dataset contains output examples
%	\begin{flushleft}
%		\begin{itemize}
%		\item \textbf{Classification} The output is a category
%		\item \textbf{Regression} The output is a number
%		\end{itemize}
%		\end{flushleft}
%	\item \textbf{Unsupervised learning} (or \textit{clustering}) The dataset does not contain output examples
%	\item \textbf{Reinforcement learning} Maximize reward
%	\end{itemize}
%	ML is data-driven
%\end{frame}

%\begin{frame}[plain]{Introduction}{Machine Learning (II)}
%	\note{Comparar data-driven con programacion clasica}
%	\begin{center}
%	\vspace{-0.3cm}
%		\includegraphics[width=0.44\linewidth]{figs/lifecycle.png}
%	\end{center}
%\end{frame}


\subsection{History}
\begin{frame}{Introduction}{History}
    \begin{columns}
 	   \column{.80\textwidth}

	\begin{itemize}
	\item[1888] \textit{Ramón y Cajal}. Discovery of biological neurons
	\item[1943] \textit{McCulloch \& Pitts}. First neural network designers
	\item[1949] \textit{Hebb}. First learning rule
	\item[1958] \textit{Rosenblatt}. Perceptron
	\item[1969] \textit{Minsky \& Papert}.  Perceptron limitation - Death of ANN
	\note{Single layer perceonptrons cannot represent (learn) simple functions such as XOR
	Multi-layer of non-linear units may have greater power but there is no learning rule for such nets
	Scaling problem: connection weights may grow infinitely}
	\item[1986] \textit{Rumelhart et al}. Re-emergence of ANN: Backpropagation
	\note{Backpropagation learning for multi-layer feed forward nets (with non-linear, differentiable node functions)}
	\item[2012] CNNs popularity - AlexNet - Rise of Deep Learning
	\item[2014] \textit{Goodfellow et al.} Generative Adversarial Networks (GANs)
	\item[2021] \textit{OpenAI}. Dall-e 2
	\item[2022] Large Language Models (ChatGPT, GPT-3, new Bing)
	\item[2023] Multimodal Large Language Models (GPT-4)
	\item[20xx] ... AGI?
	\end{itemize}

 	   \column{.20\textwidth}
		\includegraphics[width=\linewidth]{figs/neuron-ryc.jpg}
	   \end{columns}
\end{frame}

\subsection{Biological neurons}
%\begin{frame}{Introduction}{Structure of neurons (I)}
%    \vspace{-0.3cm}
%    \begin{columns}
% 	   \column{.50\textwidth}
%		\begin{tabular}{r|r}\hline
%		\sc{Animal} & \sc{Neurons} \\\hline
%		Sponge & 0 \\
%		Roundworm & 302 \\
%		Jellyfish & 800 \\
%		Ant & 250,000 \\
%		Cockroach & 1,000,000 \\
%		Frog & 16,000,000 \\
%		Mouse & 71,000,000 \\
%		Cat & 760,000,000 \\
%		Macaque & 6,376,000,000 \\
%		Human & 86,000,000,000 \\
%		Elephant & 267,000,000,000 \\\hline
%		\end{tabular}

% 	   \column{.50\textwidth}
%	   \begin{block}{Human brain}
%		Neuron switching time: ~ 0.001 s\\
%  		Synapsis: ~10-100 thousand\\
%   		Scene recognition time: ~0.1 s
%	\end{block}

%    \begin{center}
%	        \includegraphics[width=0.9\linewidth]{figs/power.jpg}\\
%	        \tiny{\href{http://www.sjef.nu/a-basic-introduction-to-singularity-skepticism/}{(Source)}}
%	\end{center}
%    \end{columns}
%\end{frame}

\begin{frame}{Introduction}{Biological neurons (I)}
	%\centering\includegraphics[width=0.7\linewidth]{figs/neuronabio.png}\\
	A neuron has a cell body (soma) ...
		\begin{itemize}
		\item ... a branching \textit{input} structure (dendrite) and 
		\item ... a branching \textit{output} structure (axon)
		\item ... an axon termination named \textit{synapses}
		\end{itemize}
	An \textit{action potential} (or signal) may propagate from dentrites to synapses
		\begin{itemize}
		\item The synapses release a chemical named \textit{neurotransmitter}
		\item Given enougth neurotransmitters, a new neuron can fire
		\end{itemize}

	\medskip

	\begin{columns}[c]
	    \column{.1\textwidth}
	    \column{.50\textwidth}
			\includegraphics[width=\linewidth]{figs/neuron-bio.png}
	    \column{.3\textwidth}
			\includegraphics[width=\linewidth]{figs/sinapsis.jpg}
	    \column{.1\textwidth}
    \end{columns}
\end{frame}

\begin{frame}{Introduction}{Biological neurons (II)}
	\begin{columns}[c]
	    \column{.7\textwidth}
		A neuron only fires if its input signal exceeds a threshold
		\begin{itemize}
		\item Good connections allowing a large signal
		\item Slight connections allowing a weak signal
		\item Synapses may be either excitatory or inhibitory
		\end{itemize}

		Synapses vary in strength
		\begin{itemize}
		\item Biological learning involves setting that strength
		\end{itemize}
		Biological neurons often are organized in layers

	\column{.3\textwidth}
		\includegraphics[width=\linewidth]{figs/cortex.png}
	        \centering \tiny{\href{https://en.wikipedia.org/wiki/Cerebral_cortex}{(Source)}}
    \end{columns}
\end{frame}

\section{Artificial neurons}

\subsection{Definition}

\begin{frame}{Artificial neurons}{Definition (I)}

    TLU: Threshold logic unit\\

    \input{figs/neuron.tex}

	\bigskip
    \begin{columns}
 	   \column{.70\textwidth}
		\begin{description}
		\item[$a_i$] Input
		\item[$w_{i}$] Weight of input $j$
		\item[$\theta$] Threshold
		\item[$g$] Activation function
		\end{description}

 	   \column{.30\textwidth}
	    \begin{block}{Neuron model (TLU)}
	   \vspace{-0.5cm}
	   \begin{equation*}
	   y=g\left( \sum_{i} w_{i} a_i \right)
	   \end{equation*}
	   \end{block}
    \end{columns}
\end{frame}

\begin{frame}{Artificial neurons}{Definition (II)}
	\begin{columns}
	\column{0.60\textwidth}
	The idealized activation function is a step function
	\begin{equation*}
	g(x) =
	  \begin{cases}
	      	1  & \text{if } \sum_{i=1}^{N} w_i x_i > \theta\\
	  	0  & \text{otherwise}\\
	  \end{cases}
	\end{equation*}
	The step function is rarely used in practice

	\column{0.30\textwidth}
    \begin{tikzpicture}[scale=0.4]
      \begin{axis}[ 
          xlabel=$x$,
          ylabel={$g(x)$},
		  xtick={2},
		  xticklabels={$\theta$}
      ] 
        %\addplot[mark=none] {2/(1+e^(-2*x))-1}; 
		\addplot[mark=none, red] coordinates {
			(-2,0)
			(2,0)
			(2,1)
			(6,1)
		};
        %\addplot {x^2 - x +4}; 
      \end{axis}
    \end{tikzpicture}
	\column{0.10\textwidth}
 	\end{columns}

\end{frame}

\begin{frame}{Artificial neurons}{Definition (III)}
	A single neuron can be used for \textit{linear binary classification}
	\begin{itemize}
	\item Computes linear combination of inputs
	\item If the output exceeds the threshold, it assigns a positive class
	\item ... othershise it assigns a negative class
	\end{itemize}

	Comparable to logistic regression or SVM
\end{frame}


\subsection{Logical gates with a neuron}
\begin{frame}{Artificial neurons}{Logical gates with a neuron}
    \begin{columns}
 	   \column{.30\textwidth}
	   		\centering AND\\
            \input{figs/ann-and.tex}

			 \begin{tabular}{ccc}\hline
			 $a_1$ & $a_2$ & $y$ \\\hline
			 $0$ & $0$ & $0$ \\
			 $0$ & $1$ & $0$ \\
			 $1$ & $0$ & $0$ \\
			 $1$ & $1$ & $1$ \\
			 \end{tabular}
 	   \column{.30\textwidth}
	   		\centering OR\\
            \input{figs/ann-or.tex}

			 \begin{tabular}{ccc}\hline
			 $a_1$ & $a_2$ & $y$ \\\hline
			 $0$ & $0$ & $0$ \\
			 $0$ & $1$ & $1$ \\
			 $1$ & $0$ & $1$ \\
			 $1$ & $1$ & $1$ \\
			 \end{tabular}
 	   \column{.30\textwidth}
	   		\centering NOT\\\bigskip
            \input{figs/ann-not.tex}
			\vspace{1cm}
			 \begin{tabular}{cc}\hline
			 $a_1$ & $y$ \\\hline
			 $0$ & $1$ \\
			 $1$ & $0$ \\
			 \end{tabular}
    \end{columns}
	\bigskip
	\centering \href{https://github.com/dfbarrero/dataCourse/raw/master/ann/artificialNeuron.xlsx}{(A neuron in Excel)}
\end{frame}

\begin{frame}{Artificial neurons}{Definition of neuron (alternative version)}
    \input{figs/neuron-alternative.tex}

	\bigskip
    \begin{columns}
 	   \column{.70\textwidth}
		\begin{description}
		\item[$a_i$] Input
		\item[$w_{i}$] Weight of input $j$
		\item[$w_0$] Bias
		\item[$g$] Activation function
		\end{description}

 	   \column{.30\textwidth}
	   \begin{block}{Neuron model}
	   \vspace{-0.5cm}
	   \begin{equation*}
	   y=g\left( \sum_{i} w_{i} a_i \right)
	   \end{equation*}
	   \end{block}
    \end{columns}
\end{frame}

\begin{frame}{Artificial neurons}{Example of biased neuron}
	AND logical gate with a biased input

	\centering \input{figs/ann-and-biased.tex}

	\bigskip

	\centering \begin{tabular}{ccc|c}\hline
	$a_0$ & $a_1$ & $a_2$& Output\\\hline
	1    & 0     & 0     & 0\\
	1    & 0     & 1     & 0\\
	1    & 1     & 0     & 0\\
	1    & 1     & 1     & 1\\\hline
	\end{tabular}
\end{frame}

\subsection{Activation functions}
\begin{frame}{Artificial neurons}{Activation functions: Sigmoid function}
	Also known as the logistic function
	\begin{itemize}
		\item Biological motivation
		\item S-shaped, continuous and everywhere differentiable
		\item Asymptotically approach saturation points
		\item Derivative fast computation
        	\item Range $\in [0, 1]$
	\end{itemize}

	\medskip

	\begin{columns}[T]
 	   \column{.60\textwidth}

    		\begin{tikzpicture}[scale=0.5]
      			\begin{axis}[ 
          			xlabel=$x$,
          			ylabel={$g(x) = \frac{1}{1+e^{-x}}$}
      			] 
        		\addplot[mark=none, red] {1/(1+e^(-x))}; 
      			\end{axis}
    		\end{tikzpicture}
                      
                      
 	   \column{.40\textwidth}
		\vspace{-0.5cm}

		\begin{block}{Sigmoid function}
		\begin{equation*}
		g(x) = \frac{1}{1+e^{-x}}
		\end{equation*}
		\begin{equation*}
		g'(x) = g(x) (1-g(x))
		\end{equation*}
		\end{block}
    	\end{columns}
\end{frame}

\begin{frame}{Artificial neurons}{Activation functions: Tanh function}
    \begin{itemize}
	\item Asymptotically approach saturation points
        \item Range $\in [-1, 1]$
        \item Bigger derivative than sigmoid (faster training)
    \end{itemize}

    \begin{columns}[T]
 	\column{.50\textwidth}

     	    \begin{tikzpicture}[scale=0.5]
      		\begin{axis}[ 
          		xlabel=$x$,
          		ylabel={$g(x) = \tanh(x)$}
      		] 
        	\addplot[mark=none,red] {2/(1+e^(-2*x))-1}; 
        	%\addplot {x^2 - x +4}; 
      		\end{axis}
    		\end{tikzpicture}
                 
 	\column{.50\textwidth}
	    \begin{block}{Tanh function}
		\vspace{-0.5cm}

	        \begin{equation*}
	        g(x) = \tanh(x) = \frac{2}{1+e^{-2x}}-1
	        \end{equation*}
	        \begin{equation*}
           	g'(x) = 1 - g(x)^2
	        \end{equation*}
	    \end{block}
    \end{columns}
\end{frame}

\begin{frame}{Artificial neurons}{Activation functions: Softmax function}
	\begin{itemize}
		\item Generalization of the logistic function
		\item Usually used in the output layer in classification problems
		\item Asymptotically approach saturation points
	\end{itemize}

	\begin{columns}[T]
 	   \column{.60\textwidth}
	    \begin{block}{Softmax function}
	        \vspace{-0.25cm}

	        \begin{equation*}
	        g(\pmb z)_j = \frac{e^{z_j}}{\sum_{k=1}^{K}e^{z_k}} \text{ for } j=1, ...,K
	        \end{equation*}
	        with  \textbf{z} a K-dimensional vector
	    \end{block}
    \end{columns}
\end{frame}

\begin{frame}{Artificial neurons}{Other activation functions}
	\begin{columns}[T]
 	   \column{.30\textwidth}

		\centering Linear function
	    	\smallskip

    		\begin{tikzpicture}[scale=0.4]
      		\begin{axis}[ 
          		xlabel=$x$,
          		ylabel={$g(x) = x$}
      		] 
        		\addplot[mark=none,red] {x}; 
      		\end{axis}
    		\end{tikzpicture}

	   	\begin{itemize}
		   \item Used in regression
           \item Last layer in regression
	   	\end{itemize}
 
 	   \column{.30\textwidth}
	   	\centering Rectified Linear (ReLU)
	    	\medskip

    		\begin{tikzpicture}[scale=0.4]
      		\begin{axis}[ 
          		xlabel=$x$,
          		ylabel={$g(x)$}
      		] 
		\addplot[mark=none, red] coordinates {
			(-6,0)
			(0,0)
			(6,6)
		};
      		\end{axis}
    		\end{tikzpicture}


	   	\begin{itemize}
		   \item Faster derivate
		   \item Popular in DL
	   	\end{itemize}

 	   \column{.30\textwidth}
	    \centering Leaky ReLU
	    \medskip

    		\begin{tikzpicture}[scale=0.4]
      		\begin{axis}[ 
          		xlabel=$x$,
          		ylabel={$g(x)$}
      		] 
		\addplot[mark=none, red] coordinates {
			(-6,-0.25)
			(0,0)
			(6,6)
		};
      		\end{axis}
    		\end{tikzpicture}

	   	\begin{itemize}
		   \item More informative derivate
		   \item Popular in DL
	   	\end{itemize}
    \end{columns}

    \bigskip
    The lack of non-linear activation function makes a network a simple linear regression
\end{frame}

\subsection{The Perceptron}
\begin{frame}{Artificial neurons}{The Perceptron}
	The Perceptron is a very simple ANN architecture
	\begin{itemize}
	\item Disclaimer: different defintions of Perceptron
	\item Proposed by Rosenblatt in 1957
	\end{itemize}

	Perceptron: Layer of TLUs connected to all the inputs
	\begin{itemize}
	\item \textit{Input layer} contains special passthrough neurons
	\item Multilabel classification
	\end{itemize}

	\begin{center}
	\includegraphics[width=0.9\linewidth]{figs/perceptron.jpg}\\
	\centering \tiny{\href{https://hackmd.io/@imkushwaha/perceptron}{(Source)}}
	\end{center}
\end{frame}

\subsection{Learning limits}
\begin{frame}{Artificial neurons}{Learning limits (I)}
	\begin{center}
	\includegraphics[width=0.9\linewidth]{figs/linear2.png}
	\end{center}
	Problem: A single perceptron only can solve linearly separable problems
\end{frame}

\begin{frame}{Artificial neurons}{Learning limits (II)}
	XOR cannot be implemented with a perceptron
	\begin{center}
	\includegraphics[width=0.7\linewidth]{figs/xor.jpg}
	\end{center}
	Solution: Stack several perceptrons 
\end{frame}

\section{The Multilayer Perceptron}
\subsection{Definition}

\begin{frame}{The Multilayer Perceptron}{Definition (I)}
	Neurons are arranged in \alert{layers} of TLU neurons

	\begin{description}
	\item[\textit{Input}] Which consists of our data
	\item[\textit{Output}] Which are the net outcome
	\item[\textit{Hidden}] (Optional) No direct interaction
	\end{description}
	Multilayer Perceptron, or simply \alert{MLP}
	\bigskip

        \input{figs/ann.tex}
\end{frame}

%\begin{frame}{The Multilayer Perceptron}{Definition (II)}
%    \begin{columns}
% 	\column{.6\textwidth}
%	The input layer
%	\begin{itemize}
%	\item Introduces input values into the network
%	\item No activation function or other processing
%	\end{itemize}
%	The hidden layer(s)
%	\begin{itemize}
%	\item Perform classification of features
%	\item Two hidden layers are sufficient to solve any problem
%	%\item Features imply more layers may be better
%	\end{itemize}
%	The output layer
%	\begin{itemize}
%	\item Functionally just like the hidden layers
%	\item Outputs are passed on to the world outside the neural network
%	\end{itemize}

% 	\column{.4\textwidth}
%	\includegraphics[width=\linewidth]{figs/network2.png}
%	\tiny{\href{https://tex.stackexchange.com/questions/540287/tikz-randomly-drop-connections-in-neural-network}{(Source)}}
%    \end{columns}
%\end{frame}

\begin{frame}{The Multilayer Perceptron}{Definition (II)}
    \begin{center}
    	\includegraphics[width=0.6\paperheight]{figs/mlp.png}\\
		\tiny \href{https://www.oreilly.com/library/view/neural-networks-and/9781492037354/ch01.html}{(Source)}
    \end{center}
\end{frame}

%\subsection{Separability}
%\begin{frame}{Feedforward Networks}{Separability}
%    \begin{center}
%	    \includegraphics[width=0.9\linewidth]{figs/sep.png}
%    \end{center}

%	\centering \href{https://github.com/dfbarrero/aiCourse/raw/master/ann/artificialNeuron.xlsx}{(A MLP in Excel)}
%\end{frame}




%\begin{frame}{Artificial Neural Networks}{What is a artificial neural network (III)}
%\begin{frame}[plain]{}{}
%\vspace{-0.2cm}
%\begin{center}
%    \begin{columns}
% 	   \column{.2\textwidth}
%	\href{http://www.asimovinstitute.org/neural-network-zoo/}{(More info)}
% 	   \column{.8\textwidth}
%	\includegraphics[width=0.7\linewidth]{figs/neuralnetworks.png}
%	\end{columns}
%\end{center}
%\end{frame}

\subsection{MLP for regression}

\begin{frame}{The Multilayer Perceptron}{MLP for regression (I)}
	One output neuron for each output dimension\\
	\centering\includegraphics[width=0.9\linewidth]{figs/control.png}
\end{frame}

\begin{frame}{The Multilayer Perceptron}{MLP for regression (II)}
	\centering\includegraphics[width=0.9\linewidth]{figs/robot.png}
\end{frame}

\begin{frame}{The Multilayer Perceptron}{MLP for regression (III)}
	You usually do not want to limit the output
	\begin{itemize}
		\item Output layer with linear activation
		\item ReLu allowed for strictly positive output
	\end{itemize}

	Loss function as in any regression
	\begin{itemize}
		\item Mean Squared Error (MSE) by default
		\item Mean Absolute Error (MAE) is less sensitive to outlayers
	\end{itemize}
	A loss function is an error function used to train / evaluate a network
\end{frame}

\subsection{MLP for classification}

\begin{frame}{The Multilayer Perceptron}{MLP for classification (I)} 
	Binary classification
	\begin{itemize}
		\item One output neuron
		\item Sigmoid activation
		\item The output can be interpreted as a positive class probability
	\end{itemize}

	\centering \input{figs/ann.tex}
\end{frame}

\begin{frame}{The Multilayer Perceptron}{MLP for classification (II)} 
    \begin{columns}
 	\column{.5\textwidth}
	\begin{flushleft}
	Multilabel classification
	\begin{itemize}
		\item One output neuron per label
		\item Labels are not mutually exclusive
		\item Sigmoid activation
	\end{itemize}
	%Cross-entropy loss function
	\end{flushleft}

 	\column{.5\textwidth}
    	\centering \includegraphics[width=0.8\linewidth]{figs/multilabel.jpg}\\
    \end{columns}

    \bigskip 

    \centering \includegraphics[width=0.7\linewidth]{figs/multilabel-example.jpg}\\
		\tiny \href{https://www.analyticsvidhya.com/blog/2021/07/demystifying-the-difference-between-multi-class-and-multi-label-classification-problem-statements-in-deep-learning/}{(Source)}
\end{frame}

\begin{frame}{The Multilayer Perceptron}{MLP for classification (III)} 
    \begin{columns}
 	\column{.5\textwidth}
	Multi-class classification
	\begin{itemize}
		\item Mutually exclusive label $\Rightarrow$ Probabilities are not independent
		\item One output neuron per label
		\item Softmax activation
	\end{itemize}
	
	 \medskip

    	\centering \includegraphics[width=0.6\linewidth]{figs/multiclass.jpg}\\
		\tiny \href{https://www.analyticsvidhya.com/blog/2021/07/demystifying-the-difference-between-multi-class-and-multi-label-classification-problem-statements-in-deep-learning/}{(Source)}

 	\column{.5\textwidth}
    	\centering \includegraphics[width=0.6\paperheight]{figs/mlp-softmax.png}\\
		\tiny \href{https://www.oreilly.com/library/view/neural-networks-and/9781492037354/ch01.html}{(Source)}
	\end{columns}

\end{frame}

\begin{frame}{The Multilayer Perceptron}{MLP for classification (IV): Example 1} 
    	\centering \includegraphics[width=0.8\paperheight]{figs/mlp-multiclass.jpg}\\
		\tiny \href{https://towardsdatascience.com/creating-a-multilayer-perceptron-mlp-classifier-model-to-identify-handwritten-digits-9bac1b16fe10}{(Source)}
\end{frame}

\begin{frame}{The Multilayer Perceptron}{MLP for classification (IV): Example 2} 
    \begin{columns}
 	   \column{.5\textwidth}
	\centering\includegraphics[width=\linewidth]{figs/world.png}
 	   \column{.5\textwidth}
	\centering\includegraphics[width=\linewidth]{figs/worldann.png}
    \end{columns}
    \centering \scriptsize\href{https://en.wikibooks.org/wiki/Cyberbotics\%27\_Robot\_Curriculum/}{(Source)}
\end{frame}

\begin{frame}{The Multilayer Perceptron}{Autoencoders}
	\centering\includegraphics[width=0.75\linewidth]{figs/autoencoder.png}\\\smallskip
	\centering\includegraphics[width=0.3\linewidth]{figs/autoencoder2.png}\\
	\scriptsize\href{https://blog.keras.io/building-autoencoders-in-keras.html}{(Source)}
\end{frame}

\subsection{Demo}
\begin{frame}{The Multilayer Perceptron}{Demo}
    \begin{center}
	\href{http://playground.tensorflow.org/}{(Online demo)}
    \end{center}
\end{frame}

%\subsection{Topologies}
%\begin{frame}{Artificial Neural Networks}{Topologies (I)}
%    Acyclic Networks
%    \begin{itemize}
%        \item Without directed cycles 
%        \item Easy to analyze
%    \end{itemize}
%    Recurrent Networks
%    \begin{itemize}
%        \item With directed cycles
%        \item Much harder to analyze
%        \item Potentially unstable
%    \end{itemize}
%    Modular nets
%    \begin{itemize}
%        \item Consists of several modules
%        \item Each module is itself an ANN
%        \item Sparse connections between modules
%    \end{itemize}
%\end{frame}

%\begin{frame}{Artificial Neural Networks}{Topologies (II)}
%	Asymmetric fully connected networks
%		\begin{itemize}
%		\item Every node is connected to every other node
%		\item Connection may be excitatory (positive), inhibitory (negative), or irrelevant (0)
%		\item Most general
%		\end{itemize}
%		\vspace{-0.7cm}
%		\begin{center}
	%\includegraphics[width=0.5\linewidth]{figs/connected.png}
%	\end{center}
%		\vspace{-0.5cm}
%	Symmetric fully connected nets
%		\begin{itemize}
%		\item Weights are symmetric ($w_{ij}$ = $w_{ji}$)
%		\end{itemize}
%\end{frame}

%\subsection{Layered networks}
%\begin{frame}{Network architecture}{Layered networks (I)}
%		\begin{itemize}
%		\item Nodes are partitioned into subsets, called layers
%		\item No connections from nodes in layer $j$ to those in layer $k$ if $j > k$
%		\end{itemize}

%    \begin{columns}
% 	   \column{.50\textwidth}
%	\centering\includegraphics[width=\linewidth]{figs/layered.png}
% 	   \column{.50\textwidth}
%	   \begin{itemize}
%	   \item Inputs are applied to nodes in layer 0
%	   \item Nodes in input layer without computation
%	   \end{itemize}
%	   \end{columns}
%\end{frame}

%\begin{frame}{Network architecture}{Layered networks (II)}
%	\begin{center}
%	\alert{Perceptron}: ANN whose input is directly contected with its output
%	\includegraphics[width=0.5\linewidth]{figs/network.png}
%	\end{center}
%\end{frame}


\section{Gradient Descent}
\subsection{Motivation}

\begin{frame}{Gradient Descent}{Motivation (I)}
    In supervised learning we have the target outputs ...
	\begin{itemize}
	\item ... so we can compare them with the observed one
	\end{itemize}

	\begin{columns}[c]
 	   \column{.10\textwidth}
 	   \column{.50\textwidth}
            \input{figs/ann-optimization.tex}

 	   \column{.40\textwidth}
            \begin{table}[]
            \centering
            \begin{tabular}{lll|l|l}\hline
		    $x_1$ & $x_2$ & $x_3$ & $\hat{y}$ & $y$ \\
               \hline
              $1.1$ & $2.5$ & $4.5$ & $0.2$ & $-0.1$   \\
              $0.9$ & $2.4$ & $1.2$ & $0.5$ & $0.4$  \\
              $1.0$ & $2.0$ & $9.9$ & $0.4$ & $1.2$ \\\hline
            \end{tabular}
	    \end{table}
 	   \column{.10\textwidth}
    \end{columns}

	\alert{Loss function} (\textit{función de pérdida}): Measure of the error
        \begin{itemize}
		\item Any error measure can be used, there are many available
		\item Usually MSE or MAE (among others ...)
        \end{itemize}

	\begin{equation*}
		MSE = \frac{1}{n} \sum_i(y_i-\hat{y}_i)^2 \Rightarrow MSE = f(\vec{\theta})
	\end{equation*}
	where $\vec{\theta}$ is our network (its weights and biases)
\end{frame}

\begin{frame}{Gradient Descent}{Motivation (II)}
	Problem: Determine $\vec{\theta}$ that minimizes $f(\vec{\theta})$
	\begin{itemize}
		\item This is a classical optimization problem
		\item Any optimization algorithm can be used
		\item ... in AI, optimization means search
	\end{itemize}

	In DL, our network may have millions of parameters
	\begin{itemize}
		\item We do know analytically $f(\vec{\theta})$
		\item Optimization based on gradients: \alert{Gradient Descent}
	\end{itemize}
	Gradient Descent is a general optimization algorithm
	\begin{itemize}
		\item Not limited to train neural networks
		\item Widely used, for instance, to fit lineal models
	\end{itemize}
\end{frame}

\subsection{Overview}

{\blackSlide
\begin{frame}[plain]{Gradient Descent}{Overview}
	\centering\includegraphics[width=\linewidth]{figs/mountain.jpg}\\
	\scriptsize\href{https://flickr.com/photos/pnglife/2362494258/in/photolist-8vGnwi-Mp7XNw-ozKpZv-4ALpeC-axW8KB-23DBJYt-frJNKt-yzPoCX-4LqDab-axf3zv-or4KtC-oqHW4q-axf5yp-23YRq9h-VNJWrs-8McrVM-dNbJXY-5sDfpy-o3SjX-qF1rGP-cfAqTh-YhNXcj-pECdqe-cfAqLE-BRziN1-22w1wBv-6rR9NW-f5iic4-82Vd2C-hzy676-HG1zW6-pT8N-RDfrcf-axf8x8-WKv575-w9JSfr-pT4H-bNwyCT-Cxjb7-bUef42-frJQ4v-Am2QCd-axXouW-axhMph-VqaQum-g78JD3-qtRE5V-egjam3-f9YgJ4-SVGSpe/}{(Source)}
\end{frame}
}

\begin{frame}{Gradient Descent}{Gradient Descent (I)}
	Calculate the gradient of the loss function with respect weights
	\begin{itemize}
		\item Adjust weights along gradient direction
		\item Gradient provides the direction
		\item $\eta$ is the \alert{learning rate}
	\end{itemize}

	\begin{columns}[c]
 	   \column{.50\textwidth}

		\begin{block}{Gradient descent}
		\begin{algorithmic}[1]
		\State $\vec{\theta} \gets random()$
		\While{Not converged}
			\ForAll {$\theta_i \in \vec{\theta} $}
			\State $\theta_{i} \gets \theta_i - \eta \frac{\partial}{\partial \theta_i} f(\vec{\theta})$
			\EndFor
		\EndWhile
		\end{algorithmic}
		\end{block}

 	   %%\column{.10\textwidth}
 	   \column{.40\textwidth}
		\includegraphics[width=\linewidth]{figs/descent.png}
	\end{columns}
\end{frame}

\begin{frame}{Gradient Descent}{Gradient Descent (II)}
	\begin{columns}[c]
 	   \column{.50\textwidth}
		Alternative notation: 
		\begin{itemize}
		\item[] $\vec{\theta} = \vec{\theta} - \eta \nabla_{\theta} f(\vec{\theta})$
		\end{itemize}

 	   \column{.50\textwidth}
		\centering
		\includegraphics[width=\linewidth]{figs/gradient_descent.png}\\
		\scriptsize \href{http://uc-r.github.io/gbm_regression}{(Source)}
	\end{columns}
\end{frame}

\subsection{Learning rate}
\begin{frame}{Gradient Descent}{Learning rate}
	The learning rate is a an important hyperparameter
	\begin{itemize}
		\item Small learning rate $\Rightarrow$ Slow convergence
		\item Large learning rate $\Rightarrow$ Jump across the valley
	\end{itemize}

	\bigskip

	\centering
	\includegraphics[width=0.45\linewidth]{figs/nn_learning.png}
	\includegraphics[width=0.45\linewidth]{figs/nn_learning2.png}\\
	\scriptsize \href{http://www.turingfinance.com/misconceptions-about-neural-networks/}{(Source)}
\end{frame}

\subsection{Gradient Descent problems}

\begin{frame}{Gradient Descent}{Gradient Descent problems (I)}
	GD is sensitive to features scaling\\
	\centering \includegraphics[width=0.8\linewidth]{figs/gd-scaling.png}\\
	\scriptsize \href{https://www.researchgate.net/figure/Gradient-descent-with-and-without-input-feature-scaling-Geron-2019-On-the-left-are_fig4_357829499}{(Source)}
\end{frame}

\begin{frame}{Gradient Descent}{Gradient Descent problems (II)}
    \begin{columns}
	   \column{.40\textwidth}
		Potential problems
		\begin{itemize}
		\item Local minima
		\item Flat plateau
		\item Oscillation
		\item Missing good minima
		\end{itemize}

		GD uses the whole dataset
		\begin{itemize}
		\item It is slow ... 
		\item ... inviable in practice
		\end{itemize}
	   \column{.60\textwidth}
	 \begin{center}
	\includegraphics[width=\linewidth]{figs/problems.png}
 	\end{center}
   \end{columns}
\end{frame}

\subsection{Stochastic Gradient Descent}
\begin{frame}{Gradient Descent}{Stochastic Gradient Descent (I)}
	SGD approximates the gradient sampling the dataset
	\begin{description}
		\item[On-line] One sample (Stochastic Gradient Descent, SGD)
		\item[Mini-batch] Several samples (named mini-batches)
		\item[Batch] All the samples (Gradient Descent)
	\end{description}

   	Computations are faster but gradient computations looses accuracy

	\centering

    \begin{columns}
    \column{0.50\textwidth}
	\centering \includegraphics[width=0.9\linewidth]{figs/sdg.png}\\
	\scriptsize \href{https://wikidocs.net/3413}{(Source)}
    \column{0.50\textwidth}
	\centering \includegraphics[width=0.9\linewidth]{figs/sgd-comparison.png}\\
	    \scriptsize \href{https://github.com/tuitet/Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow-3rd-Edition/blob/main/04_training_linear_models.ipynb}{(Source)}
    \end{columns}
\end{frame}

\begin{frame}{Gradient Descent}{Stochastic Gradient Descent(II)}
    In practice we use mini-batch SGD
    \begin{itemize}
	 \item GPUs reduce dramatically computation time
    \end{itemize}

    \begin{columns}
    \column{0.20\textwidth}
    \column{0.30\textwidth}
	\includegraphics[width=\linewidth]{figs/4090.jpg}

    \column{0.50\textwidth}
	\includegraphics[width=\linewidth]{figs/jetson.jpg}
   \end{columns}

    The batch size is one of the most important hyperparameters 
    \begin{itemize}
	\item Best performance with large batch sizes
    \item Batch size limited by VRAM (GPU RAM)
	\item Erratic gratients (or event randomness) could help to scape from local minima
    \end{itemize}
\end{frame}

\begin{frame}{Gradient Descent}{Stochastic Gradient Descent (III)}
    Each iteration is named \alert{epoch}
    \begin{itemize}
	\item Usually, an epoch involves the algorithm to visit the whole training set
	\item We really want to visualize the \textit{learning curve}
    \end{itemize}

    \begin{columns}
    \column{0.50\textwidth}
	\includegraphics[width=\linewidth]{figs/loss.png}
	\centering \scriptsize \href{https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/}{(Source)}

    \column{0.50\textwidth}
	\includegraphics[width=\linewidth]{figs/keras-output.png}\\
    \end{columns}
\end{frame}

\begin{frame}{Gradient Descent}{Stochastic Gradient Descent (IV)}
    There are advanced tools to visualize learning curves, among other cool metrics
    \begin{itemize}
        \item The most widely known is TensorBoard
    \end{itemize}

	\includegraphics[width=0.8\linewidth]{figs/tensorboard.png}
\end{frame}



\subsection{Early stopping}
\begin{frame}{Gradient Descent}{Early stopping}
	\textit{Early stopping}: Stop training when the network begins to overfit
            \begin{itemize}
                \item Compare the loss in train and test to detect overfitting
            \end{itemize}

            \begin{center}
			\includegraphics[width=0.6\linewidth]{figs/validation.png}\\
			\scriptsize \href{https://datascience.stackexchange.com/questions/61/why-is-overfitting-bad-in-machine-learning}{(Source)}
            \end{center}
\end{frame}

\section{Backpropagation}
\begin{frame}{Backpropagation}
	Efficient algorithm to compute gradients
        \begin{itemize}
		\item Proposed in 1986
		\item First practical ANN training algorithm
                \item It uses the chain rule to propagate errors
		\item Automatic computation of gradients
		\item It creates a computation graph (TensorFlow takes its name from this)
        \end{itemize}

	Implicit in ANN/DL packages
	\begin{itemize}
		\item You will find it as SGD
		\item ANN/DL packages name the optimization algorithm as 'optimizers'
	\end{itemize}
\end{frame}

\begin{frame}{Backpropagation}
	\begin{columns}
	\column{0.50\textwidth}
	\begin{block}{Backpropagation}
	\begin{enumerate}
	\item \textbf{Feed-forward step}. Feed input, one mini-batch at a time. Compute output and error
	\item \textbf{Feed-backward step}. Compute individual contribution to error (gradients) using the chain rule
	\item \textbf{Adjust weights}. Perform a Gradient Descent step using the computed gradients
	\end{enumerate}
	\end{block}

	\column{0.50\textwidth}
		%\includegraphics[width=\linewidth]{figs/network2.png}
		%\tiny{\href{https://tex.stackexchange.com/questions/540287/tikz-randomly-drop-connections-in-neural-network}{(Source)}}
		\centering \input{figs/ann-optimization.tex}
	\end{columns}
\end{frame}

\section{Other optimization algorithms}
\subsection{Momentum optimization}

\begin{frame}{Other optimization algorithms}{Momentum optimization}
    SGD does not take care about past gradients
	\begin{equation*}
		\vec{\theta} \leftarrow \vec{\theta} - \eta \nabla_{\theta} J(\vec{\theta})
	\end{equation*}
    Usually, a \alert{momentum} vector is introduced as
	\begin{equation*}
		\vec{m} \leftarrow \beta \vec{m} - \eta \nabla_\theta J(\vec{\theta})
        \end{equation*}
	\begin{equation*}
		\vec{\theta} \leftarrow \vec{\theta} + \vec{m}
        \end{equation*}
    where ...
    \begin{itemize}
	\item $\eta$ is the learning rate
	\item $\beta$ is the momentum strength
		\begin{itemize}
        		\item If $\beta = 0$ then gradient descent
			\item $\beta=0.9$ uses to be a good default
		\end{itemize}
    \end{itemize}

    \href{http://distill.pub/2017/momentum/}{(On-line demo)}
\end{frame}

\begin{frame}{Training algorithms}{Other optimization algorithms}
	Learning rate / momentum  adaptative methods
	\begin{itemize}
		\item Nesterov Accelerated Gradient - Modified momentum
		\item AdaGrad - Adaptative Gradient Algorithm
		\item RMSProp - Root Mean Square Propagation
		\item Adam - Adaptive Moment Estimation
			\begin{itemize}
				\item Adaptative learning rate
				\item Default choice in real-word problems
			\end{itemize}
	\end{itemize}
\end{frame}

\subsection{Second order optimization algorithms}
\begin{frame}{Training algorithms}{Second order optimization algorithms}
	\begin{columns}
	\column{0.50\textwidth}
		Other second derivative-based optimization algorithms
		\begin{itemize}
			\item Newton's method
			\item Quasi-Newton's method
			\item Levenberg-Marquardt method
			\item Conjugate Gradient
		\end{itemize}
		They are never used in DL
	\column{0.50\textwidth}
		\begin{center}
			\includegraphics[width=\linewidth]{figs/memory-speed.png}\\
			\scriptsize \href{https://www.neuraldesigner.com/blog/5_algorithms_to_train_a_neural_network}{(Source)}
		\end{center}
	\end{columns}
\end{frame}

\end{document}



