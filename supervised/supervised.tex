%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MUW Presentation
% LaTeX Template
% Version 1.0 (27/12/2016)
%
% License:
% CC BY-NC-SA 4.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
% Created by:
% Nicolas Ballarini, CeMSIIS, Medical University of Vienna
% nicoballarini@gmail.com
% http://statistics.msi.meduniwien.ac.at/
%
% Customized for UAH by:
% David F. Barrero, Departamento de Automática, UAH
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[10pt,compress]{beamer} % Change 10pt to make fonts of a different size
\mode<presentation>

\usepackage[spanish]{babel}
\usepackage{fontspec}
\usepackage{tikz}
\usepackage{etoolbox}
\usepackage{xcolor}
\usepackage{xstring}
\usepackage{listings}

% Custom packages
\usepackage{standalone}
\usepackage{multicol}
\usepackage{multirow} % Confusion matrix
\usepackage{tikz}
\usepackage{pgfplots}
\def\layersep{2.5cm}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows,shapes}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
 

\usetheme{UAH}
\usecolortheme{UAH}
\setbeamertemplate{navigation symbols}{} 
\setbeamertemplate{caption}[numbered]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Presentation Info
\title[Supervised learning]{Supervised learning}
\author{\asignatura\\\carrera}
\institute{}
\date{Departamento de Automática}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Descomentar para habilitar barra de navegación superior
\setNavigation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Configuración de logotipos en portada
%% Opacidad de los logotipos
\newcommand{\opacidad}{1}
%% Descomentar para habilitar logotipo en pié de página de portada
\renewcommand{\logoUno}{Images/isg.png}
%% Descomentar para habilitar logotipo en pié de página de portada
%\renewcommand{\logoDos}{Images/CCLogo.png}
%% Descomentar para habilitar logotipo en pié de página de portada
%\renewcommand{\logoTres}{Images/ALogo.png}
%% Descomentar para habilitar logotipo en pié de página de portada
%\renewcommand{\logoCuatro}{Images/ELogo.png}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% FOOTLINE
%% Comment/Uncomment the following blocks to modify the footline
%% content in the body slides. 


%% Option A: Title and institute
\footlineA
%% Option B: Author and institute
%\footlineB
%% Option C: Title, Author and institute
%\footlineC
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Use this block for a blue title slide with modified footline
{\titlepageBlue
    \begin{frame}
        \titlepage
    \end{frame}
}

\institute{\asignatura}

\begin{frame}[plain]{}
   \begin{block}{Objectives}
      \begin{enumerate}
         \item Extend supervised learning algorithms
         \item Apply supervised learning to real-world problems
      \end{enumerate} 
   \end{block}

   \begin{block}{Bibliography}
    \begin{itemize}
        \item M\"uller, Andreas C., Guido, Sarah. \textit{Introduction to Machine Learning with Python}. O'Reilly. 2016
    \end{itemize}
   \end{block}
   All figures have been taken from \url{https://github.com/amueller/introduction_to_ml_with_python/blob/master/02-supervised-learning.ipynb}
\end{frame}

{
\disableNavigation{white}
\begin{frame}[shrink]{Table of Contents}

 	\frametitle{Table of Contents}
  	\begin{multicols}{2}
  		\tableofcontents
    \end{multicols}

 %\frametitle{Table of Contents}
 %\tableofcontents
  % You might wish to add the option [pausesections]
\end{frame}
}

\section[Generalization]{Generalization, overfitting and underfitting}
\begin{frame}{Generalization, overfitting and underfitting}
    Generalization: accurate predictions on unseen data
    \begin{itemize}
	\item i.e. there is no overfitting neither underfitting
	\item Depends on model  complexity and data variability
    \end{itemize}

    \medskip

    \centering \includegraphics[width=0.6\linewidth]{figs/overfitting_underfitting_cartoon.png}
    \tiny{\href{https://github.com/amueller/introduction_to_ml_with_python/blob/master/02-supervised-learning.ipynb}{(Source)}}
\end{frame}



\section{k-Nearest Neighbors}
\subsection{k-NN classification}

\begin{frame}{k-Nearest Neighbors}{k-NN classification (I)}

     k-NN (k-Nearest Neighbors): Likely, the simplest classifier
	\begin{itemize}
		\item Given a data point, it takes its $k$ closests neighbors
		\item Same prediction than its neighbors
	\end{itemize}

    \begin{columns}
 	   \column{.40\textwidth}
        \centering 1-NN\\
	        \includegraphics[width=\textwidth]{figs/1-nn.png}
 	   \column{.40\textwidth}
         \centering   3-NN\\
	        \includegraphics[width=\textwidth]{figs/3-nn.png}
    \end{columns}

    k-NN does not generate a model
    \begin{itemize}
		\item The whole dataset must be stored
	\end{itemize}
    $k$ uses to be an odd number (1-NN, 3-NN, 5-NN, ...)
\end{frame}

\begin{frame}{k-Nearest Neighbors}{k-NN classification (II)}
    \centering 
	\includegraphics[width=0.7\textwidth]{figs/knnboundary.png}

    \flushleft

    $k$ determines the model complexity
    \begin{itemize}
        \item Smoother boundaries in larger $k$ values
        \item Model complexity decreases with $k$
        \item If $k$ equals the number of samples, k-NN always predicts the most frequent class
    \end{itemize}
    How to figure out the best $k$?
\end{frame}

\begin{frame}{k-Nearest Neighbors}{k-NN classification (III)}
    \centering 
	\includegraphics[width=0.5\textwidth]{figs/k-complexity.png}
\end{frame}

\IfStrEq{\modo}{muii}{
    \subsection{Scikit-Learn}

    \begin{frame}{k-Nearest Neighbors classifier}{Scikit-learn}
        \begin{exampleblock}{\texttt{sklearn.neighbors.KNeighborsClassifier}}
         \medskip

         \begin{columns}[T]
 	        \column{.01\textwidth}
 	        \column{.49\textwidth}
                Constructor arguments:
                \begin{itemize}
                    \item \texttt{n\_neighbors}: int, default=5
                    \item \texttt{metric}: string, default='minkowski'
                    \item \texttt{p}: int, default=2 ($p=1$ Manhatan distance, $p=2$ euclidean distance)
                \end{itemize}

 	        \column{.49\textwidth}
                Attributes:
                \begin{itemize}
                    \item \texttt{classes\_}: ndarray (n\_samples)
                \end{itemize}
            \end{columns}

            \medskip

            Methods: \texttt{fit()}, \texttt{predict()}
        \end{exampleblock}

        \medskip

        \centering \href{https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html}{(Scikit-Learn reference)}

    \end{frame}
}{}



\subsection{kNN regression}

\begin{frame}{k-Nearest Neighbors}{kNN regression (I)}
    \begin{columns}
 	   \column{.50\textwidth}
        \begin{block}{k-NN regression}
         Given a data point
        \begin{enumerate}
            \item Take the $k$ closest data points
            \item Predict same target value (1-NN) or averate target value (k-NN)
        \end{enumerate}
        \end{block}

        Performace is measured with a regression metric, by default, $R^2$

 	   \column{.40\textwidth}
        \centering 1-NN\\
	        \includegraphics[width=\textwidth]{figs/1-nn-reg.png}
         \centering   3-NN\\
	        \includegraphics[width=\textwidth]{figs/3-nn-reg.png}
    \end{columns}
\end{frame}

\begin{frame}{k-Nearest Neighbors}{kNN regression (II)}
    \includegraphics[width=\textwidth]{figs/knn-boundary-reg.png}

    $k$ determines boundary smoothness
    \begin{enumerate}
        \item With $k=1$, prediction visits all data points
        \item With large $k$ values, fit is worse
    \end{enumerate}
\end{frame}


\IfStrEq{\modo}{muii}{
    \subsection{Scikit-Learn}

    \begin{frame}{k-Nearest Neighbors regressor}{Scikit-learn}
        \begin{exampleblock}{\texttt{sklearn.neighbors.KNeighborsRegressor}}
         \medskip

         \begin{columns}[T]
 	        \column{.01\textwidth}
 	        \column{.49\textwidth}
                Constructor arguments:
                \begin{itemize}
                    \item \texttt{n\_neighbors}: int, default=5
                    \item \texttt{metric}: string, default='minkowski'
                    \item \texttt{p}: int, default=2 ($p=1$ Manhatan distance, $p=2$ euclidean distance)
                \end{itemize}

 	        \column{.49\textwidth}
                Attributes:
            \end{columns}

            \medskip

            Methods: \texttt{fit()}, \texttt{predict()}
        \end{exampleblock}

        \medskip

        \centering \href{https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html}{(Scikit-Learn reference)}

    \end{frame}
}{}




\subsection{Summary}
\begin{frame}{k-Nearest Neighbors}{Summary}
	\begin{center}
	\begin{tabular}{cp{3cm}p{3cm}}\hline
	 	\texttt{Hyperparameters}  & \texttt{Advantages}  & \texttt{Disadvantages} \\\hline
	 	$k$                       & Simple               & Slow with large datasets  \\
	 	Distance                  & Baseline             & Bad performance with hundreds or more attributes  \\
                                  &                      & No model \\
                                  &                      & Dataset must be stored in memory \\
	 	\hline
	\end{tabular}
	\end{center}
\end{frame}

\section{Linear models}
\subsection{Ordinary least squares}
\begin{frame}{Linear models}{Linear regression (I)}
     Lineal regression assumes a linear relationship among variables
	\begin{itemize}
		\item This limitation can be easely overcome
		\item Surprisingly good results in high dimensional spaces
	\end{itemize}

    \begin{columns}
 	   \column{.60\textwidth}
        \begin{block}{Lineal regression}
            $y = a_0 + a_1 x_1  + a_2 x_2 + \dots + a_n x_n$
        \end{block}

 	   \column{.40\textwidth}
		\begin{figure}
	        \includegraphics[width=\textwidth]{figs/regression.png}
		\end{figure}
    \end{columns}

\end{frame}

%\begin{frame}{Linear regression (II)}
%    Example: Does money make people happier? (example from (G\'eron, 2017))
%    \bigskip

%    \begin{columns}
%       \column{.40\textwidth}
%        \begin{tabular}{ccc}\hline
%            \texttt{Country} & \texttt{GDP} & \texttt{\textit{LS}} \\\hline
%            Hungary& 12,240  & \textit{4.9}   \\
%            Korea   & 27,195  & \textit{5.8}   \\
%            France & 37,675  & \textit{6.5}   \\
%            Australia & 50,962 & \textit{7.3} \\
%            USA     & 55,805  & \textit{7.2}   \\
%            \hline
%        \end{tabular}\\
%        LS =Life satisfaction

%       \column{.60\textwidth}
%            \begin{exampleblock}{Linear regression}
%                \centering \includegraphics[width=0.9\linewidth]{Images/scattergdpregression.png}
%                \vspace{-0.4cm}
%                \begin{equation*}
%                life\_satisfaction = \theta_0 + \theta_1 \times GDP\_per\_capita
%                \end{equation*}
%                \vspace{-0.2cm}
%            \end{exampleblock}
%    \end{columns}
%\end{frame}

\begin{frame}{Linear models (II)}
	Several methods to fit coefficients
	\begin{itemize}
		\item Ordinary Least Squares (OLS)
        \item Generalized Least Squares (GSL)
        \item Weighted Least Squares (WLS)
		\item Generalized Least Squares with AR Covariance Structure (GLSAR)
	\end{itemize}

	\alert{Regularization}: Term that penalizes complexity
	\begin{itemize}
		\item L1 (Lasso regression) 
		\item L2 (Ridge regression)
		\item ElasticNet: L1 and L2
	\end{itemize}

    \begin{columns}
 	   \column{.3\textwidth}
    		\begin{block}{Lasso}
            	$\lambda \sum_j^n \beta_j^2$
        	\end{block}

	   \column{.3\textwidth}
     		\begin{block}{Ridge}
            	$\lambda \sum_j^n |\beta_j|$
        	\end{block}
	   \column{.4\textwidth}
     		\begin{block}{ElasticNet}
			$\alpha  \sum_j^n \beta_j^2 + (1-\alpha) \sum_j^n |\beta_j|$
        	\end{block}
    \end{columns}

\end{frame}

\subsection{Ridge regression}
\subsection{Lasso regression}
\subsection{ElasticNet}
\subsection{Linear models for classification}

\IfStrEq{\modo}{muii}{
    \subsection{Scikit-Learn}

    \begin{frame}{Linear models}{Scikit-learn}
        TODO
        \begin{exampleblock}{\texttt{sklearn.cluster.AgglomerativeClustering}}
         \medskip

         \begin{columns}[T]
 	        \column{.01\textwidth}
 	        \column{.49\textwidth}
                Constructor arguments:
                \begin{itemize}
                    \item \texttt{linkage}: {‘ward’, ‘complete’, ‘average’, ‘single’}
                \end{itemize}

 	        \column{.49\textwidth}
                Attributes:
                \begin{itemize}
                    \item \texttt{n\_clusters}: int
                    \item \texttt{labels\_}: ndarray (n\_samples)
                \end{itemize}
            \end{columns}

            \medskip

            Methods: \texttt{fit()}, \texttt{fit\_predict()}
        \end{exampleblock}

        \medskip

        \centering \href{https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html}{(Scikit-Learn reference)}

    \end{frame}
}{}

\subsection{Summary}
\begin{frame}{Linear models}{Summary}
	\begin{center}
	\begin{tabular}{cp{3cm}p{3cm}}\hline
	 	\texttt{Hyperparameters}  & \texttt{Advantages}  & \texttt{Disadvantages} \\\hline
	 	                          &                               &   \\
	 	\hline
	\end{tabular}
	\end{center}
\end{frame}



\section{Naive Bayes Classifiers}

\begin{frame}{Naive Bayes Classifiers}
    TODO
\end{frame}

\IfStrEq{\modo}{muii}{
    \subsection{Scikit-Learn}

    \begin{frame}{Naive Bayes Classifiers}{Scikit-learn}
        \begin{exampleblock}{\texttt{sklearn.cluster.AgglomerativeClustering}}
         \medskip

         \begin{columns}[T]
 	        \column{.01\textwidth}
 	        \column{.49\textwidth}
                Constructor arguments:
                \begin{itemize}
                    \item \texttt{linkage}: {‘ward’, ‘complete’, ‘average’, ‘single’}
                \end{itemize}

 	        \column{.49\textwidth}
                Attributes:
                \begin{itemize}
                    \item \texttt{n\_clusters}: int
                    \item \texttt{labels\_}: ndarray (n\_samples)
                \end{itemize}
            \end{columns}

            \medskip

            Methods: \texttt{fit()}, \texttt{fit\_predict()}
        \end{exampleblock}

        \medskip

        \centering \href{https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html}{(Scikit-Learn reference)}

    \end{frame}
}{}

\subsection{Summary}
\begin{frame}{Naive Bayes Classifiers}{Summary}
	\begin{center}
	\begin{tabular}{cp{3cm}p{3cm}}\hline
	 	\texttt{Hyperparameters}  & \texttt{Advantages}  & \texttt{Disadvantages} \\\hline
	 	                          &                               &   \\
	 	\hline
	\end{tabular}
	\end{center}
\end{frame}

\section{Decission Trees}

\begin{frame}{Decission Trees}
    TODO
\end{frame}

\IfStrEq{\modo}{muii}{
    \subsection{Scikit-Learn}

    \begin{frame}{Decission Trees}{Scikit-learn}
        \begin{exampleblock}{\texttt{sklearn.cluster.AgglomerativeClustering}}
         \medskip

         \begin{columns}[T]
 	        \column{.01\textwidth}
 	        \column{.49\textwidth}
                Constructor arguments:
                \begin{itemize}
                    \item \texttt{linkage}: {‘ward’, ‘complete’, ‘average’, ‘single’}
                \end{itemize}

 	        \column{.49\textwidth}
                Attributes:
                \begin{itemize}
                    \item \texttt{n\_clusters}: int
                    \item \texttt{labels\_}: ndarray (n\_samples)
                \end{itemize}
            \end{columns}

            \medskip

            Methods: \texttt{fit()}, \texttt{fit\_predict()}
        \end{exampleblock}

        \medskip

        \centering \href{https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html}{(Scikit-Learn reference)}

    \end{frame}
}{}

\subsection{Summary}
\begin{frame}{Decission Trees}{Summary}
	\begin{center}
	\begin{tabular}{cp{3cm}p{3cm}}\hline
	 	\texttt{Hyperparameters}  & \texttt{Advantages}  & \texttt{Disadvantages} \\\hline
	 	                          &                               &   \\
	 	\hline
	\end{tabular}
	\end{center}
\end{frame}

\section{Ensembles of Decision Trees}

\begin{frame}{Ensembles of Decision Trees}
    TODO
\end{frame}

\IfStrEq{\modo}{muii}{
    \subsection{Scikit-Learn}

    \begin{frame}{Ensembles of Decision Trees}{Ensembles of Decision Trees : Scikit-learn}
        \begin{exampleblock}{\texttt{sklearn.cluster.AgglomerativeClustering}}
         \medskip

         \begin{columns}[T]
 	        \column{.01\textwidth}
 	        \column{.49\textwidth}
                Constructor arguments:
                \begin{itemize}
                    \item \texttt{linkage}: {‘ward’, ‘complete’, ‘average’, ‘single’}
                \end{itemize}

 	        \column{.49\textwidth}
                Attributes:
                \begin{itemize}
                    \item \texttt{n\_clusters}: int
                    \item \texttt{labels\_}: ndarray (n\_samples)
                \end{itemize}
            \end{columns}

            \medskip

            Methods: \texttt{fit()}, \texttt{fit\_predict()}
        \end{exampleblock}

        \medskip

        \centering \href{https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html}{(Scikit-Learn reference)}

    \end{frame}
}{}

\subsection{Summary}
\begin{frame}{Ensembles of Decision Trees}{Summary}
	\begin{center}
	\begin{tabular}{cp{3cm}p{3cm}}\hline
	 	\texttt{Hyperparameters}  & \texttt{Advantages}  & \texttt{Disadvantages} \\\hline
	 	                          &                               &   \\
	 	\hline
	\end{tabular}
	\end{center}
\end{frame}

\section{Support Vector Machines}

\begin{frame}{Support Vector Machines}
    TODO
\end{frame}

\subsection{Kernelized Support Vector Machines}
\begin{frame}{Support Vector Machines}{Kernelized Support Vector Machines}
    TODO
\end{frame}

\IfStrEq{\modo}{muii}{
    \subsection{Support Vector Machines}{Scikit-Learn}

    \begin{frame}{Support Vector Machines}{Scikit-learn}
        \begin{exampleblock}{\texttt{sklearn.cluster.AgglomerativeClustering}}
         \medskip

         \begin{columns}[T]
 	        \column{.01\textwidth}
 	        \column{.49\textwidth}
                Constructor arguments:
                \begin{itemize}
                    \item \texttt{linkage}: {‘ward’, ‘complete’, ‘average’, ‘single’}
                \end{itemize}

 	        \column{.49\textwidth}
                Attributes:
                \begin{itemize}
                    \item \texttt{n\_clusters}: int
                    \item \texttt{labels\_}: ndarray (n\_samples)
                \end{itemize}
            \end{columns}

            \medskip

            Methods: \texttt{fit()}, \texttt{fit\_predict()}
        \end{exampleblock}

        \medskip

        \centering \href{https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html}{(Scikit-Learn reference)}

    \end{frame}
}{}

\subsection{Summary}
\begin{frame}{Support Vector Machines}{Summary}
	\begin{center}
	\begin{tabular}{cp{3cm}p{3cm}}\hline
	 	\texttt{Hyperparameters}  & \texttt{Advantages}  & \texttt{Disadvantages} \\\hline
	 	                          &                               &   \\
	 	\hline
	\end{tabular}
	\end{center}
\end{frame}






\section{A}
\subsection{b}

\begin{frame}{A}{B}
    TODO
\end{frame}

\IfStrEq{\modo}{muii}{
    \subsection{A: Scikit-Learn}

    \begin{frame}{A}{B: Scikit-learn}
        \begin{exampleblock}{\texttt{sklearn.cluster.AgglomerativeClustering}}
         \medskip

         \begin{columns}[T]
 	        \column{.01\textwidth}
 	        \column{.49\textwidth}
                Constructor arguments:
                \begin{itemize}
                    \item \texttt{linkage}: {‘ward’, ‘complete’, ‘average’, ‘single’}
                \end{itemize}

 	        \column{.49\textwidth}
                Attributes:
                \begin{itemize}
                    \item \texttt{n\_clusters}: int
                    \item \texttt{labels\_}: ndarray (n\_samples)
                \end{itemize}
            \end{columns}

            \medskip

            Methods: \texttt{fit()}, \texttt{fit\_predict()}
        \end{exampleblock}

        \medskip

        \centering \href{https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html}{(Scikit-Learn reference)}

    \end{frame}
}{}

\subsection{A: Summary}
\begin{frame}{A}{B: Summary}
	\begin{center}
	\begin{tabular}{cp{3cm}p{3cm}}\hline
	 	\texttt{Hyperparameters}  & \texttt{Advantages}  & \texttt{Disadvantages} \\\hline
	 	                          &                               &   \\
	 	\hline
	\end{tabular}
	\end{center}
\end{frame}






\subsection{ARIMA}
\begin{frame}{Algorithms}{ARIMA (I)}
    \begin{columns}
 	   \column{.5\textwidth}
	   AR: Autoregressive model
	    \begin{itemize}
		\item Current observation depends on the last $p$ observations
		\item Long term memory
	    \end{itemize}

	   \column{.5\textwidth}
		\begin{block}{AR(p)}
			$X_t = c+\sum_{i=1}^p \phi_i X_{t-1}+\epsilon_t$
        	\end{block}
    \end{columns}

    \smallskip

    \begin{columns}
 	   \column{.5\textwidth}
	   MA: Moving Average model
	    \begin{itemize}
		\item Current observation linearly depends on the last $q$ innovations
		\item Short term memory
	    \end{itemize}

	   \column{.5\textwidth}
		\begin{block}{MA(q)}
			$X_t =  \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + ... + \theta_q \epsilon_{t-q}$
        	\end{block}
    \end{columns}

    \bigskip

    ARMA model = AR + MA
	\begin{itemize}
		\item ARMA(p, q): Two hyperparameters, p and q
	\end{itemize}

\end{frame}

\begin{frame}{Algorithms}{ARIMA (II)}
	ARIMA = AR + i + MA (AR integrated MA)
    \begin{itemize}
	\item ARIMA(p, d, q)
	\item Three integer parameters: p, q and d (in practice, low order models)
    \end{itemize}

    \centering \includegraphics[width=0.6\linewidth]{figs/arima.png}

    \tiny{\href{https://itnext.io/understanding-the-forecasting-algorithm-stlf-model-29d74b3a0336?gi=282b647b24a7}{(Source)}}

    \normalsize
	\flushleft
    \alert{autoarima}: search over p, q and d
\end{frame}



\end{document}
