%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MUW Presentation
% LaTeX Template
% Version 1.0 (27/12/2016)
%
% License:
% CC BY-NC-SA 4.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
% Created by:
% Nicolas Ballarini, CeMSIIS, Medical University of Vienna
% nicoballarini@gmail.com
% http://statistics.msi.meduniwien.ac.at/
%
% Customized for UAH by:
% David F. Barrero, Departamento de Automática, UAH
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[10pt,compress]{beamer} % Change 10pt to make fonts of a different size
\mode<presentation>

\usepackage[spanish]{babel}
\usepackage{fontspec}
\usepackage{tikz}
\usepackage{etoolbox}
\usepackage{xcolor}
\usepackage{xstring}
\usepackage{listings}

% Custom packages
\usepackage{svg}
\usepackage{standalone}
\usepackage{multicol}
\usepackage{multirow} % Confusion matrix
\usepackage{tikz}
\usepackage{pgfplots}
\def\layersep{2.5cm}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows,shapes}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
 

\usetheme{UAH}
\usecolortheme{UAH}
\setbeamertemplate{navigation symbols}{} 
\setbeamertemplate{caption}[numbered]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Presentation Info
\title[Unsupervised learning]{Unsupervised learning}
\author{\asignatura\\\carrera}
\institute{}
\date{Departamento de Automática}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Descomentar para habilitar barra de navegación superior
\setNavigation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Configuración de logotipos en portada
%% Opacidad de los logotipos
\newcommand{\opacidad}{1}
%% Descomentar para habilitar logotipo en pié de página de portada
\renewcommand{\logoUno}{Images/isg.png}
%% Descomentar para habilitar logotipo en pié de página de portada
%\renewcommand{\logoDos}{Images/CCLogo.png}
%% Descomentar para habilitar logotipo en pié de página de portada
%\renewcommand{\logoTres}{Images/ALogo.png}
%% Descomentar para habilitar logotipo en pié de página de portada
%\renewcommand{\logoCuatro}{Images/ELogo.png}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% FOOTLINE
%% Comment/Uncomment the following blocks to modify the footline
%% content in the body slides. 


%% Option A: Title and institute
\footlineA
%% Option B: Author and institute
%\footlineB
%% Option C: Title, Author and institute
%\footlineC
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Use this block for a blue title slide with modified footline
{\titlepageBlue
    \begin{frame}
        \titlepage
    \end{frame}
}

\institute{\asignatura}

\begin{frame}[plain]{}
   \begin{block}{Objectives}
      \begin{enumerate}
         \item Extend unsupervised learning algorithms
         \item Apply unsupervised learning to real-world problems
      \end{enumerate} 
   \end{block}

   \begin{block}{Bibliography}
	\begin{itemize}
        \item G\'eron, Aur\'elien. \textit{Hands-On Machine Learning with Scikit-Learn, Keras \& TensorFlow}. O'Reilly. 2020
        \item M\"uller, Andreas C., Guido, Sarah. \textit{Introduction to Machine Learning with Python}. O'Reilly. 2016
	\end{itemize}
   \end{block}
\end{frame}

{
\disableNavigation{white}
\begin{frame}[shrink]{Table of Contents}

 	\frametitle{Table of Contents}
  	\begin{multicols}{2}
  		\tableofcontents
    \end{multicols}

 %\frametitle{Table of Contents}
 %\tableofcontents
  % You might wish to add the option [pausesections]
\end{frame}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Clustering}
%{
%\sectionheaderWhite %Enclose the frame with {} and add this command for white background in section header
%\begin{frame}{Clustering}{K-means, agglomerative clustering, DBSCAN and GMM}
%\end{frame}
%}


\section{Clustering}

\begin{frame}{Clustering}
	Set of unsupervised techniques that identify groups of data (named \alert{clusters})
    \begin{itemize}
        \item No universal definition of cluster: Centroid, medoid, dense regions, etc
    \end{itemize}

    Applications
	\begin{itemize}
		\item Customer segmentation, data analysis, dimensionality reduction, anomaly detection, semi-supervised learning, search engines, image segmentation
	\end{itemize}

    Main algorithms
	\begin{itemize}
		\item K-means, DBScan, GMM, hierarchical clustering, ...
	\end{itemize}
\end{frame}

\section{K-means}
\subsection{Overview}
\begin{frame}{K-means}{Overview}
    \begin{columns}
 	   \column{.5\textwidth}
       \centering Original data\\
		\includegraphics[width=\textwidth]{figs/kmeans-1.png}
 	   \column{.5\textwidth}
       \centering Clustered data\\
		\includegraphics[width=\textwidth]{figs/kmeans-2.png}
    \end{columns}

    \centering \tiny{\href{https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html}{(Source)}}

    \normalsize
    \begin{flushleft}
    In k-means, clusters are identified by a \alert{centroid}
    \end{flushleft}
\end{frame}

\subsection{K-means algorithm}
\begin{frame}{K-means}{K-means algorithm (I)}
    \begin{columns}
 	   \column{.5\textwidth}
       \begin{block}{K-means algorithm}
       \begin{enumerate}
        \item Set $k$ random centroids
        \item Assign each data point to its closest centroid
        \item Recompute centroids
        \item Go to 2 until no point reassignment
       \end{enumerate}
       \end{block}

       k is an hyperparameter
       \begin{itemize}
        \item Number of clusters
       \end{itemize}

 	   \column{.5\textwidth}
		\includegraphics[width=\textwidth]{figs/kmeans.jpeg}\\

    \centering \tiny{\href{https://dendroid.sk/2011/05/09/k-means-clustering/}{(Source)}}
    \end{columns}

\end{frame}

\begin{frame}{K-means}{K-means algorithm (II)}
    New data points are assigned to its closest centroid
    \begin{columns}
 	   \column{.5\textwidth}
		\includegraphics[width=\textwidth]{figs/kmeans-2.png}

 	   \column{.5\textwidth}
		\includegraphics[width=\textwidth]{figs/voronoi.png}
    \end{columns}
\end{frame}

\subsection{K-means limitations}
\begin{frame}{K-means}{K-means limitations}
    \begin{columns}
 	   \column{.5\textwidth}
    K-means can fail in several conditions
       \begin{itemize}
        \item Incorrect number of clusters
        \item Different clusters variance
        \item Non-spheric clusters $\Rightarrow$ normalization
       \end{itemize}
 	   \column{.5\textwidth}
	\centering \includegraphics[width=\textwidth]{figs/kmeans-fails.png}\\
    \tiny{\href{https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html}{(Source)}}
    \end{columns}
\end{frame}

\subsection{Elbow's method}
\begin{frame}{K-means}{Elbow's method}
    Election of $k$
    \begin{itemize}
        \item Not a problem when domain information is available
        \item ... that is rarely the case
    \end{itemize}

    \begin{columns}
 	   \column{.50\textwidth}
        \begin{block}{Elbow's method}
           \begin{enumerate}
            \item Select $K=1, ..., n$
            \item Visualize performance for each k
            \item Choose K where metric stabilizes
           \end{enumerate} 
        \end{block}
 	   \column{.50\textwidth}
			\begin{figure}
		        \includegraphics[width=\textwidth]{figs/elbow.png}
			\end{figure}
    \end{columns}

    Performance measures
    \begin{itemize}       
        \item Inertia: mean squared error between each instance and its closest centroid
        \item Silhouette: $(b-a) / max (a, b)$, where $a$ mean intra-cluster distance, and $b$ is the mean nearest-cluster distance
    \end{itemize}       
\end{frame}

\subsection{Application: Image segmentation}
\begin{frame}[plain]{K-means}{Application: Image segmentation}
	\includegraphics[width=0.8\textwidth]{figs/segmentation1.png}\\
        \centering \tiny{\href{https://www.kdnuggets.com/2019/08/introduction-image-segmentation-k-means-clustering.html}{(Source)}}

    \medskip

	\includegraphics[width=0.8\textwidth]{figs/segmentation2.png}\\
        \centering \tiny{\href{https://up42.com/blog/tech/satellite-image-processing-made-simple}{(Source)}}
\end{frame}

\subsection{Application: for semi-supervised learning}
\begin{frame}[plain]{K-means}{Application: Clustering for semi-supervised learning}
    Semi-supervised learning: Only a subset of the dataset is labeled
    \begin{itemize}
       \item Supervised and unsupervised learning
       \item Quite common in real-world applications (labels use to be expensive)
    \end{itemize} 

	\begin{center}
	\begin{tabular}{cccccc}\hline
	 	$f_1$     & $f_2$     & $\cdots$ & $f_n$     & $y$\\\hline
	 	$a_{1,1}$ & $a_{2,1}$ & $\cdots$ & $a_{n,1}$ & $y_1$ \\
	 	$a_{1,2}$ & $a_{2,2}$ & $\cdots$ & $a_{n,2}$ &  \\
	 	$a_{1,3}$ & $a_{2,3}$ & $\cdots$ & $a_{n,3}$ &  \\
	 	$a_{1,4}$ & $a_{2,4}$ & $\cdots$ & $a_{n,4}$ & $y_4$ \\
	 	$a_{1,5}$ & $a_{2,5}$ & $\cdots$ & $a_{n,5}$ &  \\
	 	\hline
	\end{tabular}
	\end{center}

    \begin{block}{Label propagation}
    \begin{enumerate}
       \item Obtain $k$ clusters
       \item Get a representative instance of each cluster (\alert{medoid}) measuring the distance to the centroid
       \item Label the members of each cluster with its medoid's label
    \end{enumerate} 
    \end{block}
\end{frame}

\IfStrEq{\modo}{muii}{
    \subsection{K-means: Scikit-Learn}
    \begin{frame}{Clustering}{K-means: Scikit-learn}
        \begin{exampleblock}{\texttt{class sklearn.cluster.KMeans()}}
         %\texttt{class sklearn.cluster.KMeans()}
         \medskip

         \begin{columns}[T]
 	        \column{.01\textwidth}
 	        \column{.49\textwidth}
                Constructor arguments:
                \begin{itemize}
                    \item \texttt{init}: \{‘k-means++’, ‘random’\}
                    \item \texttt{n\_init}: int, default=10
                    \item \texttt{max\_iter}: int, default=300
                \end{itemize}

 	        \column{.50\textwidth}
                Attributes:
                \begin{itemize}
                    \item \texttt{cluster\_centers\_}: ndarray (n\_clusters, n\_features)
                    \item \texttt{labels\_}: ndarray (n\_samples,)
                    \item \texttt{inertia\_}: float
                \end{itemize}
            \end{columns}

            Methods: \texttt{fit()}, \texttt{predict()}
        \end{exampleblock}

        \medskip

        \centering \href{https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html}{(Scikit-Learn reference)}
    \end{frame}
}{}

\subsection{K-means summary}
\begin{frame}[fragile]{K-means}{K-means: Summary}
	\begin{center}
	\begin{tabular}{cll}\hline
	 	\texttt{Hyperparameters}  & \texttt{Advantages}  & \texttt{Disadvantages} \\\hline
	 	                 & Fast                 & Simple shapes \\
	    $k$	             & Few hyperparameters  & Determine $k$ \\
	 	                 & Scalable             & Random initialization \\
	 	\hline
	\end{tabular}
	\end{center}
\end{frame}


\section{Other clustering algorithms}

\subsection{GMM}

\begin{frame}{Other clustering algorithms}{Gaussian Mixure Model (GMM) (I)}
	GMM is a generative clustering algorithm
	\begin{itemize}
		\item Assumes data coming from a set of multidimensional gaussian distributions
	\end{itemize}

    GMM fits a set $\{(\phi_i, \mu_i, \sigma_i)\}_{i=1,\dots, k}$, where $k$ is the number of clusters and
	\begin{itemize}
			\item $\phi$ is a weight
			\item $\mu$ is a multidimensional mean
			\item $\sigma$ is a covariance matrix
            %\item $k$ is the number of clusters (hyperparameter)
	\end{itemize}

	\bigskip
	\centering \includegraphics[width=0.41\linewidth]{figs/gmm2.png}
	\includegraphics[width=0.41\linewidth]{figs/gmm3.png}\\
	%\centering\includegraphics[width=0.31\linewidth]{figs/gmm1.png}\\
	\scriptsize\href{https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html}{(Source)}
\end{frame}

\begin{frame}{Other clustering algorithms}{Gaussian Mixure Model (GMM) (II)}
    Gaussian parameters are fit with the Expectation-Maximization (E-M) algorithm
       \begin{itemize}
            \item E-M is a generalization of K-means
       \end{itemize}

    \begin{block}{Expectation-Maximization algorithm}
    \begin{enumerate}
       \item Init parameters randomly
       \item Expectation step: Assign each instance to a cluster
            \begin{itemize}
            \item Assignment is probabilistic
            \end{itemize}
       \item Maximization step: Update cluster parameters
            \begin{itemize}
            \item Each cluster is updated using \textit{all} the data
            \item Instances contribution to a cluster parameters is weighted by the probability that it belongs to it
            \end{itemize}
       \item Go to 2
    \end{enumerate} 
    \end{block}

    GMM can be seen as a fuzzy clustering algorithm
\end{frame}

\begin{frame}{Other clustering algorithms}{Gaussian Mixure Model (GMM) (III)}
    \begin{columns}
 	    \column{.5\textwidth}
            Covariance matrices can be constrained
            \begin{itemize}
                \item Full: No restriction
                \item Spherical: Sheprical shapes, different diameters
                \item Diag: Ellipsoidal shapes, axes parallel to the coordinate system
                \item Tied: Same ellipsoidal shapes, size and orientation
            \end{itemize}

 	    \column{.5\textwidth}
	        \includegraphics[width=\textwidth]{figs/gmm-covariances.png}
    \end{columns}
\end{frame}

\subsection{GMM for anomaly detection}

\begin{frame}{Other clustering algorithms}{GMM for anomaly detection}
    GMM provides a probability of an instance to belong to a cluster
    \begin{itemize}
         \item This can be used to detect anomalies
         \item Just assign a probability threshold
    \end{itemize} 
    
    \bigskip

    \centering
	\includegraphics[width=0.45\textwidth]{figs/gmm-anomaly.png}
	\includegraphics[width=0.45\textwidth]{figs/gmm-anomaly2.png}\\
	\scriptsize\href{https://www.kaggle.com/code/albertmistu/detect-anomalies-using-gmm}{(Source)}
\end{frame}

\IfStrEq{\modo}{muii}{
    \subsection{GMM: Scikit-Learn}
    \begin{frame}{Other clustering algorithms}{GMM: Scikit-learn}
        \begin{exampleblock}{\texttt{class sklearn.mixture.GMM}}
         \medskip

         \begin{columns}[T]
 	        \column{.01\textwidth}
 	        \column{.49\textwidth}
                Constructor arguments:
                \begin{itemize}
                    \item \texttt{n\_components}: int, default=1
                    \item \texttt{covariance\_type}: \{‘full’, ‘spherical’, 'diag', 'tied'\}
                    \item \texttt{n\_iter:}: int, default=100
                    \item \texttt{n\_init}: int, default=1
                \end{itemize}

 	        \column{.49\textwidth}
                Attributes:
                \begin{itemize}
                    \item \texttt{weights\_}: (n\_components,)
                    \item \texttt{means\_}: (n\_components, n\_features)
                    \item \texttt{covars\_}: See documention
                \end{itemize}
            \end{columns}

            \medskip

            Methods: \texttt{fit()}, \texttt{predict()}, \texttt{predict\_proba()}
        \end{exampleblock}

        \medskip

        \centering \href{https://scikit-learn.org/0.16/modules/generated/sklearn.mixture.GMM.html}{(Scikit-Learn reference)}
    \end{frame}
}{}

\begin{frame}{Other clustering algorithms}{GMM: Summary}
	\begin{center}
	\begin{tabular}{cp{3cm}p{3cm}}\hline
	 	\texttt{Hyperparameters}  & \texttt{Advantages}      & \texttt{Disadvantages} \\\hline
	 	Number of clusters        & Probabilistic clustering & Number of clusters \\
	    Covariance matrix type	  & Generative model         & Gaussian data \\
	    	                      & Anomaly detection        & Sensitive to outliers \\
	 	\hline
	\end{tabular}
	\end{center}
\end{frame}

\subsection{DBSCAN}

\begin{frame}{Other clustering algorithms}{DBSCAN (I)}
	DBSCAN: Density-Based Spatial Clustering of Applications with Noise
	\begin{itemize}
		\item Identifies high density regions (dense regions) in feature space
        \item Asumtion: Clusters form dense regions separated by empty areas
	\end{itemize}
    Hyperparameters
    \begin{itemize}
        \item $min\_samples$: Minimun cluster size
        \item $\epsilon$: Radius of a neighborhood
    \end{itemize}

    Type of points
    \begin{itemize}
        \item Core instance
        \item Frontier instance
        \item Outliers
    \end{itemize}

    \vspace{-3.7cm}
	\begin{flushright} 
        \includegraphics[width=0.5\linewidth]{figs/dbscan.png}\\
	    \scriptsize\href{https://en.wikipedia.org/wiki/DBSCAN}{(Source)}\hspace{10cm}
    \end{flushright}
\end{frame}

%\begin{frame}{Other clustering algorithms}{DBSCAN (II)}
%    \begin{columns}
% 	   \column{.5\textwidth}
%       \begin{block}{DBSCAN algorithm}
%            \begin{enumerate}
%                \item For each instance, take the neighbors located within a distance of $\epsilon$
%                \item If it has $>min_samples$ neighbors, consider it as core instance
%            \end{enumerate}
%       \end{block}
%    \end{columns}
%\end{frame}

\begin{frame}{Other clustering algorithms}{DBSCAN (II)}
    \begin{columns}
 	   \column{.5\textwidth}
        \centering $\epsilon$=0.05, $min\_samples=5$\\
	    \includegraphics[width=\textwidth]{figs/dbscan1.png}\\
        
 	   \column{.5\textwidth}
       \centering $\epsilon$=0.2, $min\_samples=5$\\
	   \includegraphics[width=\textwidth]{figs/dbscan2.png}\\
    \end{columns}
    \centering \scriptsize{\href{https://github.com/Akramz/Hands-on-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow/blob/master/09.Unsupervised_learning.ipynb}{(Source)}}
\end{frame}

\IfStrEq{\modo}{muii}{
    \subsection{DBSCAN: Scikit-Learn}
    \begin{frame}{Other clustering algorithms}{DBSCAN: Scikit-learn}
        \begin{exampleblock}{\texttt{class sklearn.cluster.DBSCAN}}
         \medskip

         \begin{columns}[T]
 	        \column{.01\textwidth}
 	        \column{.49\textwidth}
                Constructor arguments:
                \begin{itemize}
                    \item \texttt{eps}: float, default=0.5
                    \item \texttt{min\_samples}: int, default=5
                \end{itemize}

 	        \column{.49\textwidth}
                Attributes:
                \begin{itemize}
                    \item \texttt{labels\_}: ndarray (n\_samples), -1 for outlayers
                    % components: Core instances in the dataset
                    \item \texttt{components\_}: ndarray (n\_core\_samples, n\_features)
                \end{itemize}
            \end{columns}

            \medskip

            Methods: \texttt{fit()}, \texttt{fit\_predict()}
        \end{exampleblock}

        \medskip

        \centering \href{https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html}{(Scikit-Learn reference)}

    \end{frame}
}{}

\subsection{DBSCAN: Summary}
\begin{frame}{Other clustering algorithms}{DBSCAN: Summary}
	\begin{center}
	\begin{tabular}{cp{3cm}p{3cm}}\hline
	 	\texttt{Hyperparameters}  & \texttt{Advantages}    & \texttt{Disadvantages} \\\hline
	 	$\epsilon$                & Unknown number of clusters  & Slower than K-means \\
	    $min\_samples$	          & Scales relatively well & No model \\
	                 	          & Almost deterministic   & Clusters with different densities \\
	                 	          & Robust to outliers     &  \\
	                 	          & Anomaly detection      &  \\
	 	\hline
	\end{tabular}
	\end{center}
\end{frame}

\subsection{Agglomerative clustering}

\begin{frame}{Other clustering algorithms}{Agglomerative clustering (I)}

    \begin{columns}
 	   \column{.8\textwidth}
            \begin{block}{Agglomerative clustering}
            \begin{enumerate}
                \item Initially, each instance forms a cluster
                \item Merge the two most similar clusters according to a metric
                \item Repeat 2 until a stop criterion is satisfied
            \end{enumerate}
            \end{block}
    \end{columns}

    \medskip
    \begin{center}
	\includegraphics[width=0.7\linewidth]{figs/agglomerative.png}\\
    \end{center}
    \medskip

    We need a similarity measure between two clusters
    \begin{itemize}
        \item \textit{Ward}: Variance within merged clusters. Leads to equally sized clusters
        \item \textit{Average}: Average distances 
        \item \textit{Complete}: Maximun distance 
        \item \textit{Single}: Minimum distance
    \end{itemize}
\end{frame}

\begin{frame}{Other clustering algorithms}{Agglomerative clustering (II)}
    Agglomerative clustering is a special case of hierarchical clustering
    
    \bigskip

	\centering Dendrogram\\ \includegraphics[width=0.5\linewidth]{figs/dendrogram.png}

	\includegraphics[width=0.7\linewidth]{figs/agglomerative.png}\\
	\scriptsize\href{https://stackoverflow.com/questions/8016313/agglomerative-clustering-in-matlab}{(Source)}
\end{frame}

\begin{frame}{Other clustering algorithms}{Agglomerative clustering (III)}
	\centering Iris dataset dendrogram\\
    \includegraphics[width=0.6\linewidth]{figs/iris-dendrogram.png}\\
	\scriptsize\href{https://scikit-learn.org/stable/auto\_examples/cluster/plot\_agglomerative\_dendrogram.html\#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py}{(Source)}
\end{frame}

\IfStrEq{\modo}{muii}{
    \subsection{Agglomerative clustering: Scikit-Learn}

    \begin{frame}{Other clustering algorithms}{DBSCAN: Scikit-learn}
        \begin{exampleblock}{\texttt{sklearn.cluster.AgglomerativeClustering}}
         \medskip

         \begin{columns}[T]
 	        \column{.01\textwidth}
 	        \column{.49\textwidth}
                Constructor arguments:
                \begin{itemize}
                    \item \texttt{linkage}: {‘ward’, ‘complete’, ‘average’, ‘single’}
                \end{itemize}

 	        \column{.49\textwidth}
                Attributes:
                \begin{itemize}
                    \item \texttt{n\_clusters}: int
                    \item \texttt{labels\_}: ndarray (n\_samples)
                \end{itemize}
            \end{columns}

            \medskip

            Methods: \texttt{fit()}, \texttt{fit\_predict()}
        \end{exampleblock}

        \medskip

        \centering \href{https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html}{(Scikit-Learn reference)}

    \end{frame}
}{}

\subsection{Agglomerative clustering: Summary}
\begin{frame}{Other clustering algorithms}{Agglomerative clustering: Summary}
	\begin{center}
	\begin{tabular}{cp{3cm}p{3cm}}\hline
	 	\texttt{Hyperparameters}  & \texttt{Advantages}  & \texttt{Disadvantages} \\\hline
	 	                 & Complex shapes                &  Slow \\
	     Similarity      & Hierarchical clustering       &  No model    \\
	 	                 &         &  \\
	 	\hline
	\end{tabular}
	\end{center}
\end{frame}

\section{Anomaly detection}

\begin{frame}{Anomaly detection}
    Two related concepts
    \begin{itemize}
        \item Outlayer detection and novelty detection
    \end{itemize}

    Adaptation of clustering and classification algorithms
    \begin{itemize}
        \item PCA, GMM, autoencoders, etc
    \end{itemize}

	\begin{center}
	\begin{tabular}{ccc}
	 	One-Class SVM & LOF    & Isolation Forest \\
	    \includegraphics[width=0.33\textwidth]{figs/oneclasssvm.png} &
	    \includegraphics[width=0.33\textwidth]{figs/lof.png} &
	    \includegraphics[width=0.33\textwidth]{figs/isolationforest.png} \\
	\end{tabular}
	\end{center}
	\centering \scriptsize\href{https://scikit-learn.org/stable/modules/outlier_detection.html}{(Source)}
\end{frame}


%\section{Dimensionality reduction}
%{
%\sectionheaderWhite %Enclose the frame with {} and add this command for white background in section header
%\begin{frame}{Dimensionality reduction}{PCA and manifold learning}
%\end{frame}
%}

\section{Dimensionality reduction}
\subsection{Main approaches}

\begin{frame}{Dimensionality reduction}{Main approaches (I)}
	Two main approaches to dimensionality reduction: Projection and \alert{manifold learning}

    \bigskip

    \centering Projection
    \begin{columns}
 	   \column{.05\textwidth}
 	   \column{.45\textwidth}
	    \includegraphics[width=\textwidth]{figs/subspace_projection.png} 
 	   \column{.45\textwidth}
	    \includegraphics[width=\textwidth]{figs/2d_projection.png} 
 	   \column{.05\textwidth}
    \end{columns}
	\scriptsize\href{https://github.com/Akramz/Hands-on-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow/blob/master/08.Dim_Reduction.ipynb}{(Source)}

    \normalsize
    \flushleft
    Algorithms: Principal Components Analysis (PCA), kernelized PCA, ...
\end{frame}

\begin{frame}{Dimensionality reduction}{Main approaches (II)}
    \centering Manifold learning
    \begin{columns}
 	   \column{.5\textwidth}
	    \includegraphics[width=\textwidth]{figs/manifold2.jpeg} 
 	   \column{.5\textwidth}
	    \includegraphics[width=\textwidth]{figs/manifold.jpeg} 
    \end{columns}
	\scriptsize\href{https://www.quora.com/What-is-manifold-learning}{(Source)}
    \normalsize
    \flushleft

    Manifold learning algorithms
    \begin{itemize}
		\item Isomap, T-distributed Stochastic Neighbor Embedding (t-SNE), Multi-dimensional Scaling (MDS), Locally Linear Embedding (LLE), ...
    \end{itemize}
\end{frame}

\subsection{PCA}

\begin{frame}{Dimensionality reduction}{Principal Components Analysis}
	PCA create a new coordinate system
    \begin{itemize}
        \item New axes capture maximum variance and are orthogonal
            \begin{itemize}
            \item They are named \textbf{principal components}
            \end{itemize}
        \item The amount of variance captured by each principal component is captured
        \item PCA does not change the original dimensionality
    \end{itemize}

    \begin{columns}
 	   \column{.60\textwidth}
			\includegraphics[width=\linewidth]{figs/2d_variance_projection.png}
    		\centering \tiny{\href{https://github.com/Akramz/Hands-on-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow/blob/master/08.Dim_Reduction.ipynb}{(Source)}}
 	   \column{.40\textwidth}
	        \includegraphics[width=\linewidth]{figs/pcavariance.png}
    		\centering \tiny{\href{https://www.kaggle.com/code/mirzarahim/introduction-to-pca-image-compression-example}{(Source)}}
    \end{columns}
\end{frame}

\begin{frame}{Dimensionality reduction}{Principal Components Analysis: Applications (I)}
	PCA application: Image compression

    \bigskip

    \begin{columns}
 	   \column{.50\textwidth}
            Original image
			\includegraphics[width=\linewidth]{figs/pcaoriginal.png}\\
 	   \column{.50\textwidth}
            Compressed image (38 dimensions)
			\includegraphics[width=\linewidth]{figs/pca150comps.png}
    \end{columns}

   	\centering \tiny{\href{https://www.kaggle.com/code/mirzarahim/introduction-to-pca-image-compression-example}{(Source)}}
\end{frame}

\begin{frame}{Dimensionality reduction}{Principal Components Analysis: Applications (II)}
	\includegraphics[width=\linewidth]{figs/pcaexample.png}\\
   	\centering \tiny{\href{https://github.com/amueller/introduction_to_ml_with_python/blob/master/03-unsupervised-learning.ipynb}{(Source)}}
\end{frame}

\begin{frame}{Dimensionality reduction}{Principal Components Analysis: Applications (III)}
	Application: Anomaly detection to predict bearing failure, vibrations of four bearings

    \smallskip

    \begin{columns}
 	   \column{.50\textwidth}
            \centering Vibration time series
			\includegraphics[width=\linewidth]{figs/bearings1.png}
 	   \column{.50\textwidth}
            \centering First component
			\includegraphics[width=\linewidth]{figs/bearingspca.png}
    \end{columns}

    \begin{columns}
 	   \column{.50\textwidth}
            \centering Reconstructed time series
			\includegraphics[width=\linewidth]{figs/bearings2.png}
 	   \column{.50\textwidth}
            \centering Reconstruction error
			\includegraphics[width=\linewidth]{figs/bearingerror.png}
    \end{columns}

   	\centering \tiny{\href{https://www.atmosera.com/blog/pca-based-anomaly-detection/}{(Source)}}
\end{frame}


%\begin{frame}{Dimensionality reduction}{Principal Components Analysis (IV)}
%	\vspace{-0.5cm}
%    \begin{columns}
% 	   \column{.70\textwidth}
%		Example: Hand-written digits recognition
%		\begin{itemize}
%			\item Images of hand-written digits
%			\item 8x8 images (64 dimensions)
%			\item $10$ digits
%			\item Classification problem
%		\end{itemize}
% 	   \column{.30\textwidth}
%			\includegraphics[width=\linewidth]{figs/zero.png}
%    \end{columns}

%    \begin{columns}
% 	   \column{.50\textwidth}
%			\includegraphics[width=\linewidth]{figs/handdigitspca.png}
% 	   \column{.50\textwidth}
%			\includegraphics[width=\linewidth]{figs/pcacomponents.png}
%    \end{columns}

%   	\centering \tiny{\href{https://github.com/amueller/introduction_to_ml_with_python/blob/master/03-unsupervised-learning.ipynb}{(Source)}}
%\end{frame}

\subsection{Kernel PCA}
\begin{frame}{Dimensionality reduction}{Kernel PCA}
    \begin{columns}
 	   \column{.50\textwidth}
	        The kernel trick applies to PCA
		    \begin{itemize}
            \item kPCA captures non-linear structures
		    \end{itemize}

 	   \column{.30\textwidth}
	        \includegraphics[width=\linewidth]{figs/swiss.jpeg}\\
 	   \column{.20\textwidth}
    \end{columns}

    \centering
	\includegraphics[width=\linewidth]{figs/kpca.png}
   	\tiny{\href{https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/}{(Source)}}
\end{frame}

\IfStrEq{\modo}{muii}{
    \subsection{PCA: Scikit-Learn}
    \begin{frame}{Dimensionality reduction}{PCA: Scikit-Learn (I)}
        \begin{exampleblock}{\texttt{sklearn.decomposition.PCA}}
         \medskip

         \begin{columns}[T]
 	        \column{.01\textwidth}
 	        \column{.49\textwidth}
                Constructor arguments:
                \begin{itemize}
                    \item \texttt{n\_components}: int, float or ‘mle’, default=None
                \end{itemize}

 	        \column{.49\textwidth}
                Attributes:
                \begin{itemize}
                    \item \texttt{components\_}: ndarray (n\_components, n\_features)
                    \item \texttt{explained\_variance\_}: ndarray (n\_components,)
                \end{itemize}
            \end{columns}

            \medskip

            Methods: \texttt{fit()}, \texttt{transform()}, \texttt{inverse\_transform()}
        \end{exampleblock}

        \medskip

        \centering \href{https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html}{(Scikit-Learn reference)}
    \end{frame}

    \begin{frame}{Dimensionality reduction}{PCA: Scikit-Learn (II)}
        \begin{exampleblock}{\texttt{sklearn.decomposition.KernelPCA}}
         \medskip

         \begin{columns}[T]
 	        \column{.01\textwidth}
 	        \column{.49\textwidth}
                Constructor arguments:
                \begin{itemize}
                     \item \texttt{n\_components}: int, default=None
                     \item \texttt{kernel}: \{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘cosine’, ‘precomputed’\}
                \end{itemize}

 	        \column{.49\textwidth}
                Attributes:
            \end{columns}

            \medskip

            Methods: \texttt{fit()}, \texttt{transform()}, \texttt{inverse\_transform()}
        \end{exampleblock}

        \medskip

        \centering \href{https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html\#sklearn.decomposition.KernelPCA}{(Scikit-Learn reference)}

    \end{frame}
}{}

\subsection{Manifold learning: Locally Linear Embedding}
\begin{frame}{Dimensionality reduction}{Manifold learning}
	Locally Linear Embedding (LLE)
    \begin{itemize}
        \item  Measures how mach each training instance linearly relates to its closest neighbors preserving local relations
    \end{itemize}

	\includegraphics[width=0.7\linewidth]{figs/lle.png}
   	\centering \tiny{\href{https://github.com/Akramz/Hands-on-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow/blob/master/08.Dim_Reduction.ipynb}{(Source)}}
\end{frame}

\subsection{Other manifold techniques}
\begin{frame}[plain]{Dimensionality reduction}{Other manifold learning techniques}
    %\begin{columns}
 	%   \column{.50\textwidth}
    \begin{itemize}
	    \item Multidimensional Scaling (MDS): Preserves distances
        \item Isomap: Preserves geodesic distance 
        \item t-Distributed Stochastic Neighbor Embedding (t-SNE):
            Preserves local distances and keep dissimilar instances apart
    \end{itemize}
        
 	%   \column{.50\textwidth}
    \centering
	\includegraphics[width=\linewidth]{figs/manifold.png}\\
   	\tiny{\href{https://scikit-learn.org/0.21/auto_examples/manifold/plot_compare_methods.html}{(Source)}}
   % \end{columns}
\end{frame}

\begin{frame}{Dimensionality reduction}{Manifold learning demo}
% Show PCA/t-SNE with Iris
% Give time for MNIST dataset
\href{http://projector.tensorflow.org/}{(Interactive demo)}
\end{frame}

\end{document}
