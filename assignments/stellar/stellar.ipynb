{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff00e8a",
   "metadata": {},
   "source": [
    "# Stellar objects identification with Machine Learning\n",
    "\n",
    "### Inteligencia Artificial para los Sistemas de Control Autónomos\n",
    "### Máster de Ciencia y Tecnología desde el Espacio\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Deal with an open Machine Learning problem.\n",
    "- Classification of tabular unbalanced data.\n",
    "- Hyperparameter optimization with grid and random search.\n",
    "- Handle outliers.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The [Sloan Digital Sky Survey](https://www.sdss.org/) (or SDSS) is, according to [Wikipedia](https://en.wikipedia.org/wiki/Sloan_Digital_Sky_Survey), \"a major multi-spectral imaging and spectroscopic redshift survey using a dedicated 2.5-m wide-angle optical telescope at Apache Point Observatory in New Mexico, United States\". It contains a huge database in which each night hundreds of gigabytes are added.\n",
    "\n",
    "In this practice we will start from a data set of stellar objects to train a model that identifies the type of object it is from among three categories: Galaxy, star or quasar. The dataset contains 100.000 instances, it is unbalanced and contains a large number of outliers.\n",
    "\n",
    "The features that the dataset contains are the following ones:\n",
    "\n",
    "*    *obj_ID* = Object Identifier, the unique value that identifies the object in the image catalog used by the CAS\n",
    "*    *alpha* = Right Ascension angle (at J2000 epoch)\n",
    "*    *delta* = Declination angle (at J2000 epoch)\n",
    "*    *u* = Ultraviolet filter in the photometric system\n",
    "*    *g* = Green filter in the photometric system\n",
    "*    *r* = Red filter in the photometric system\n",
    "*    *i* = Near Infrared filter in the photometric system\n",
    "*    *z* = Infrared filter in the photometric system\n",
    "*    *run_ID* = Run Number used to identify the specific scan\n",
    "*    *rereun_ID* = Rerun Number to specify how the image was processed\n",
    "*    *cam_col* = Camera column to identify the scanline within the run\n",
    "*    *field_ID* = Field number to identify each field\n",
    "*    *spec_obj_ID* = Unique ID used for optical spectroscopic objects (this means that 2 different observations with the same spec_obj_ID must share the output class)\n",
    "*    *class* = object class (galaxy, star or quasar object)\n",
    "*    *redshift* = redshift value based on the increase in wavelength\n",
    "*    *plate* = plate ID, identifies each plate in SDSS\n",
    "*    *MJD* = Modified Julian Date, used to indicate when a given piece of SDSS data was taken\n",
    "*    *fiber_ID* = fiber ID that identifies the fiber that pointed the light at the focal plane in each observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e684ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['obj_ID', 'alpha', 'delta', 'u', 'g', 'r', 'i', 'z', 'run_ID',\n",
       "       'rerun_ID', 'cam_col', 'field_ID', 'spec_obj_ID', 'class', 'redshift',\n",
       "       'plate', 'MJD', 'fiber_ID'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://github.com/dfbarrero/dataCourse/raw/master/assignments/stellar/stellar.zip\")\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767fe9b1",
   "metadata": {},
   "source": [
    "## Exercise: EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "505f5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91bde5b",
   "metadata": {},
   "source": [
    "## Exercise: Data cleaning\n",
    "\n",
    "Locate the outliers in the dataset and remove them. There are many methods you can use, one quite easy is to visualize a boxplot or histogram of each attribute and drop those values that are too high or low. You also should remove irrelevant or redundant attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aef1127",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481eaebe",
   "metadata": {},
   "source": [
    "## Dealing with unbalanced classes\n",
    "There are several actions that can be taken to address an unbalanced dataset, the best approach will depend on the problem and the data itself. \n",
    "\n",
    "The classical accuracy used in classification, when there is unbalanced classes, looses reliability. Imagine, for instance, a dataset with 99% of instances of a class A, and only 1% of class B. If we apply a dummy classifier that classifies everything as A, we will obtain an accuracy of 99%, which suggests excellent performance, but clearly this classifier will not be working correctly. For this reason accuracy is rarely used with unbalanced datasets.\n",
    "\n",
    "One of the most commonly used metrics with unbalanced data is F1, which is defined as the harmonic mean of the precision and recall:\n",
    "\n",
    "$F1 = 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}$.\n",
    "\n",
    "F1 is defined in a range from zero to one, with one corresponding to a perfect classification. The definition is made for binary classification problems, so it needs to be adapted to multi-label problems like the one we are dealing with. Scikit applies F1 to each class separately, obtaining several F1 values, which it then groups by means of an average. There are several methods to do this grouping. More information in [Scikit-Learn reference documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html).\n",
    "\n",
    "Another approach to deal with an unbalanced dataset is to make it a balanced one. The most straightforward way to this is to simply undersample the majority class or oversample the minority class. A variation of the latter is to add noise to the oversampling in a controlled way, so as to bring more variety to the dataset, avoiding overlearning. In the context of Deep Learning this technique is known as \"data augmentation\".\n",
    "\n",
    "<img align=\"center\" src=\"smote.webp\" width=\"300\">\n",
    "\n",
    "More advanced techniques involve synthetically generating new instances of the minority class. For example, [SMOTE](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/) is a technique of some sophistication and quite popular that creates new instances of the minority class by sampling the straight line joining a minority class instance with some close neighbor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba661b72",
   "metadata": {},
   "source": [
    "SMOTE is not supported by Sckit-Learn, we need another library named imblearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6bad953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/david/anaconda3/envs/ml37/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Requirement already satisfied: imblearn in /home/david/anaconda3/envs/ml37/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /home/david/anaconda3/envs/ml37/lib/python3.7/site-packages (from imblearn) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/david/anaconda3/envs/ml37/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.21.5)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /home/david/anaconda3/envs/ml37/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/david/anaconda3/envs/ml37/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/david/anaconda3/envs/ml37/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/david/anaconda3/envs/ml37/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45f47770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GALAXY    59445\n",
      "STAR      21594\n",
      "QSO       18961\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"https://github.com/dfbarrero/dataCourse/raw/master/assignments/stellar/stellar.zip\")\n",
    "print(df['class'].value_counts())\n",
    "\n",
    "x = df.drop(['class'], axis = 1)\n",
    "y = df.loc[:,'class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03a0421f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['GALAXY', 'QSO', 'STAR'], dtype=object), array([59445, 59445, 59445]))\n"
     ]
    }
   ],
   "source": [
    "smote= SMOTE()\n",
    "x_smote, y_smote = smote.fit_resample(x, y)\n",
    "\n",
    "print(np.unique(y_smote, return_counts=True))\n",
    "\n",
    "del x_smote, y_smote\n",
    "del df, x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba097627",
   "metadata": {},
   "source": [
    "## Exercise: Predictive modeling\n",
    "\n",
    "Train a classifier to identify if a certain object is a galaxy, quasar or star. You can use any technique of your choice to deal with the unbalanced dataset and try, at least, five classifiers. Obtain the best classifier you can without sophisticated hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "726415dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc9b0b",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization\n",
    "\n",
    "The performance of the models depends strongly on the hyperparameters, which is more clear when dealing with real-world problems. Since performance can be quantified, we can view this problem as a pure optimization problem, for which there are a multitude of techniques in AI. \n",
    "\n",
    "<center> Grid search <img align=\"center\" src=\"grid.png\" width=\"300\"></center>\n",
    "\n",
    "One of the most widely used hyperparameter optimization techniques in Machine Learning is provided by the Scikit-Learn function *GridSearchCV()*, which is provided with the hyperparameters to be optimized and a range of values to be searched. The function creates a combination of hyperparameters and trains the model with each combination, returning the combination that performs best. It can apply cross-validation if required to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23f59478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 1, 'kernel': 'linear'}\n",
      "Best score: 0.9800000000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "parameters = {'kernel':('linear', 'rbf'), \n",
    "              'C':[1, 10]}\n",
    "\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "clf.fit(iris.data, iris.target)\n",
    "\n",
    "print(f\"Best params: {clf.best_params_}\")\n",
    "print(f\"Best score: {clf.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ce31a7",
   "metadata": {},
   "source": [
    "Please observe that [*GridSearchCV()*](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) returns an object that contains the result of the search, but behaves like a model.\n",
    "\n",
    "<center>Random search <img align=\"center\" src=\"random.png\" width=\"300\"></center>\n",
    "\n",
    "Since the search space increases exponentially with the number of hyperparameters to be optimized, grid search becomes computationally intractable with some ease. A randomized search may be useful in these cases ([see *RandomizedSearchCV()*](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)), or other much more advanced methods such as bayesian optimization or even Genetic Algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06b1c72",
   "metadata": {},
   "source": [
    "## Exercise: Predictive modeling with hyperparameter optimization\n",
    "\n",
    "Get the best classifier you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55572598",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
